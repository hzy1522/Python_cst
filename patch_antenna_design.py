"""
åŸºäºPyTorchçš„è´´ç‰‡å¤©çº¿è®¾è®¡ç³»ç»Ÿ
ä¸“é—¨é’ˆå¯¹:
- è¾“å…¥: è´´ç‰‡é•¿å®½ã€GNDåšåº¦ã€ä¿¡å·çº¿åšåº¦
- è¾“å‡º: S11æœ€å°å€¼ã€å¯¹åº”é¢‘ç‡ã€è¿œåŒºåœºå¢ç›Š
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from torch.utils.data import Dataset, DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from python_hfss import *
import time
import os

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class PatchAntennaDesignSystem:
    def __init__(self, device=None):
        """åˆå§‹åŒ–è´´ç‰‡å¤©çº¿è®¾è®¡ç³»ç»Ÿ"""
        self.device = device if device else torch.device(
            'cuda' if torch.cuda.is_available() else 'cpu'
        )
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")

        # ç³»ç»Ÿå‚æ•°
        self.scaler = StandardScaler()
        self.input_dim = 4  # è´´ç‰‡é•¿å®½ã€GNDåšåº¦ã€ä¿¡å·çº¿åšåº¦
        self.output_dim = 3  # S11æœ€å°å€¼ã€å¯¹åº”é¢‘ç‡ã€è¿œåŒºåœºå¢ç›Š

        # å‚æ•°å’Œæ€§èƒ½æŒ‡æ ‡åç§°
        self.param_names = ['è´´ç‰‡é•¿åº¦(mm)', 'è´´ç‰‡å®½åº¦(mm)', 'GNDåšåº¦(mm)', 'ä¿¡å·çº¿åšåº¦(mm)']
        self.perf_names = ['S11æœ€å°å€¼(dB)', 'å¯¹åº”é¢‘ç‡(GHz)', 'è¿œåŒºåœºå¢ç›Š(dBi)']

    def load_csv_data(self, csv_file, param_cols=None, perf_cols=None):
        """
        ä»CSVæ–‡ä»¶åŠ è½½è´´ç‰‡å¤©çº¿æ•°æ®

        å‚æ•°:
        csv_file: CSVæ–‡ä»¶è·¯å¾„
        param_cols: å‚æ•°åˆ—ååˆ—è¡¨ (å¯é€‰)
        perf_cols: æ€§èƒ½æŒ‡æ ‡åˆ—ååˆ—è¡¨ (å¯é€‰)

        è¿”å›:
        X_scaled: å½’ä¸€åŒ–çš„å¤©çº¿å‚æ•°
        y: å¤©çº¿æ€§èƒ½æŒ‡æ ‡
        X_original: åŸå§‹å¤©çº¿å‚æ•°
        y_original: åŸå§‹æ€§èƒ½æŒ‡æ ‡
        """
        print(f"ä»CSVæ–‡ä»¶åŠ è½½æ•°æ®: {csv_file}")

        # è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv(csv_file)
        print(f"æ•°æ®å½¢çŠ¶: {df.shape}")
        print(f"åˆ—å: {list(df.columns)}")

        # å¦‚æœæ²¡æœ‰æŒ‡å®šåˆ—åï¼Œåˆ™ä½¿ç”¨é»˜è®¤åˆ—å
        if param_cols is None:
            param_cols = ['patch_length', 'patch_width', 'gnd_thickness', 'signal_thickness']
            print(f"ä½¿ç”¨é»˜è®¤å‚æ•°åˆ—å: {param_cols}")

        if perf_cols is None:
            perf_cols = ['s11_min', 'freq_at_s11_min', 'far_field_gain']
            print(f"ä½¿ç”¨é»˜è®¤æ€§èƒ½åˆ—å: {perf_cols}")

        # éªŒè¯åˆ—åæ˜¯å¦å­˜åœ¨
        for col in param_cols + perf_cols:
            if col not in df.columns:
                raise ValueError(f"åˆ—å '{col}' ä¸åœ¨CSVæ–‡ä»¶ä¸­")

        # æå–æ•°æ®
        X_original = df[param_cols].values
        y_original = df[perf_cols].values

        # éªŒè¯æ•°æ®ç»´åº¦
        if X_original.shape[1] != self.input_dim:
            raise ValueError(f"å‚æ•°åˆ—æ•°åº”ä¸º {self.input_dim}ï¼Œä½†å®é™…ä¸º {X_original.shape[1]}")

        if y_original.shape[1] != self.output_dim:
            raise ValueError(f"æ€§èƒ½åˆ—æ•°åº”ä¸º {self.output_dim}ï¼Œä½†å®é™…ä¸º {y_original.shape[1]}")

        # æ•°æ®å½’ä¸€åŒ–
        X_scaled = self.scaler.fit_transform(X_original)

        print(f"å‚æ•°æ•°æ®å½¢çŠ¶: {X_original.shape}")
        print(f"æ€§èƒ½æ•°æ®å½¢çŠ¶: {y_original.shape}")

        # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡ä¿¡æ¯
        print(f"\nå‚æ•°ç»Ÿè®¡:")
        for i, (name, col) in enumerate(zip(self.param_names, param_cols)):
            print(f"  {name}: å‡å€¼={X_original[:, i].mean():.3f}, æ ‡å‡†å·®={X_original[:, i].std():.3f}")

        print(f"\næ€§èƒ½æŒ‡æ ‡ç»Ÿè®¡:")
        for i, (name, col) in enumerate(zip(self.perf_names, perf_cols)):
            print(f"  {name}: å‡å€¼={y_original[:, i].mean():.3f}, æ ‡å‡†å·®={y_original[:, i].std():.3f}")

        return (torch.tensor(X_scaled, dtype=torch.float32),
                torch.tensor(y_original, dtype=torch.float32),
                X_original, y_original)

    def generate_synthetic_data(self, num_samples=5000):
        """
        ç”Ÿæˆåˆæˆçš„è´´ç‰‡å¤©çº¿æ•°æ®ç”¨äºæµ‹è¯•

        å‚æ•°:
        num_samples: æ ·æœ¬æ•°é‡

        è¿”å›:
        X_scaled: å½’ä¸€åŒ–çš„å¤©çº¿å‚æ•°
        y: å¤©çº¿æ€§èƒ½æŒ‡æ ‡
        X_original: åŸå§‹å¤©çº¿å‚æ•°
        y_original: åŸå§‹æ€§èƒ½æŒ‡æ ‡
        """
        np.random.seed(42)
        print(f"ç”Ÿæˆåˆæˆè´´ç‰‡å¤©çº¿æ•°æ®ï¼Œæ ·æœ¬æ•°: {num_samples}")

        # è´´ç‰‡å¤©çº¿å‚æ•°èŒƒå›´
        patch_length = np.random.uniform(5, 15, num_samples)  # è´´ç‰‡é•¿åº¦ 10-50mm
        patch_width = np.random.uniform(5, 15, num_samples)   # è´´ç‰‡å®½åº¦ 10-50mm
        gnd_thickness = np.random.uniform(0.01, 0.05, num_samples)  # GNDåšåº¦ 0.5-3.0mm
        signal_thickness = np.random.uniform(0.01, 0.05, num_samples)  # ä¿¡å·çº¿åšåº¦ 0.1-1.0mm

        X_original = np.column_stack([patch_length, patch_width, gnd_thickness, signal_thickness])

        # æ€§èƒ½æŒ‡æ ‡è®¡ç®—ï¼ˆåŸºäºç”µç£å­¦åŸç†çš„ç®€åŒ–æ¨¡å‹ï¼‰
        c = 3e8  # å…‰é€Ÿ

        # è°æŒ¯é¢‘ç‡è®¡ç®—
        L_meters = patch_length * 1e-3
        freq = c / (2 * L_meters * np.sqrt(4.4)) / 1e9  # å‡è®¾ä»‹ç”µå¸¸æ•°ä¸º4.4 (FR4)
        freq += np.random.normal(0, 0.2, num_samples)  # æ·»åŠ å™ªå£°

        # S11æœ€å°å€¼è®¡ç®— (ä¸å¤©çº¿å°ºå¯¸å’ŒåŒ¹é…æœ‰å…³)
        s11_min = -25 - 0.1 * (patch_length + patch_width) + np.random.normal(0, 2, num_samples)
        s11_min = np.clip(s11_min, -40, -10)  # S11èŒƒå›´é™åˆ¶åœ¨-40åˆ°-10dB

        # è¿œåŒºåœºå¢ç›Šè®¡ç®—
        gain = 2.0 + 0.02 * (patch_length + patch_width) - 0.5 * gnd_thickness + np.random.normal(0, 0.3, num_samples)
        gain = np.clip(gain, 0, 10)  # å¢ç›ŠèŒƒå›´é™åˆ¶åœ¨0-10dBi

        y_original = np.column_stack([s11_min, freq, gain])

        # æ•°æ®å½’ä¸€åŒ–
        X_scaled = self.scaler.fit_transform(X_original)

        print(f"åˆæˆæ•°æ®ç”Ÿæˆå®Œæˆ")
        print(f"å‚æ•°æ•°æ®å½¢çŠ¶: {X_original.shape}")
        print(f"æ€§èƒ½æ•°æ®å½¢çŠ¶: {y_original.shape}")

        return (torch.tensor(X_scaled, dtype=torch.float32),
                torch.tensor(y_original, dtype=torch.float32),
                X_original, y_original)

    def create_model(self, model_type='resnet'):
        """
        åˆ›å»ºè´´ç‰‡å¤©çº¿è®¾è®¡ç¥ç»ç½‘ç»œæ¨¡å‹

        å‚æ•°:
        model_type: æ¨¡å‹ç±»å‹ ('mlp', 'resnet', 'cnn', 'rnn')

        è¿”å›:
        ç¥ç»ç½‘ç»œæ¨¡å‹
        """
        if model_type == 'mlp':
            return nn.Sequential(
                nn.Linear(self.input_dim, 128),
                nn.BatchNorm1d(128),
                nn.ReLU(),
                nn.Dropout(0.2),

                nn.Linear(128, 256),
                nn.BatchNorm1d(256),
                nn.ReLU(),
                nn.Dropout(0.3),

                nn.Linear(256, 128),
                nn.BatchNorm1d(128),
                nn.ReLU(),
                nn.Dropout(0.2),

                nn.Linear(128, self.output_dim)
            ).to(self.device)

        elif model_type == 'resnet':
            # æ®‹å·®ç½‘ç»œ
            class ResidualBlock(nn.Module):
                def __init__(self, dim):
                    super().__init__()
                    self.block = nn.Sequential(
                        nn.Linear(dim, dim),
                        nn.BatchNorm1d(dim),
                        nn.ReLU(),
                        nn.Linear(dim, dim),
                        nn.BatchNorm1d(dim)
                    )
                    self.relu = nn.ReLU()

                def forward(self, x):
                    return self.relu(x + self.block(x))

            return nn.Sequential(
                nn.Linear(self.input_dim, 128),
                nn.BatchNorm1d(128),
                nn.ReLU(),

                ResidualBlock(128),
                ResidualBlock(128),

                nn.Linear(128, 256),
                nn.BatchNorm1d(256),
                nn.ReLU(),

                ResidualBlock(256),
                ResidualBlock(256),

                nn.Linear(256, self.output_dim)
            ).to(self.device)

        elif model_type == 'cnn':
            # ä¸€ç»´å·ç§¯ç½‘ç»œ
            return nn.Sequential(
                nn.Unflatten(1, (1, self.input_dim)),
                nn.Conv1d(1, 32, kernel_size=3, padding=1),
                nn.BatchNorm1d(32),
                nn.ReLU(),
                nn.MaxPool1d(2),

                nn.Conv1d(32, 64, kernel_size=3, padding=1),
                nn.BatchNorm1d(64),
                nn.ReLU(),
                nn.MaxPool1d(2),

                nn.Flatten(),
                nn.Linear(64 * (self.input_dim // 4), 128),
                nn.ReLU(),
                nn.Linear(128, self.output_dim)
            ).to(self.device)

        elif model_type == 'rnn':
            # å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆä½¿ç”¨GRUï¼‰
            class RNNModel(nn.Module):
                def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2):
                    super().__init__()
                    self.hidden_dim = hidden_dim
                    self.num_layers = num_layers

                    # GRUå±‚
                    self.gru = nn.GRU(
                        input_size=input_dim,
                        hidden_size=hidden_dim,
                        num_layers=num_layers,
                        batch_first=True,
                        bidirectional=True
                    )

                    # æ‰¹å½’ä¸€åŒ–å±‚
                    self.batch_norm = nn.BatchNorm1d(hidden_dim * 2)  # *2 for bidirectional

                    # å…¨è¿æ¥å±‚
                    self.fc1 = nn.Linear(hidden_dim * 2, 128)
                    self.fc2 = nn.Linear(128, output_dim)

                    # æ¿€æ´»å‡½æ•°å’Œ dropout
                    self.relu = nn.ReLU()
                    self.dropout = nn.Dropout(0.3)

                def forward(self, x):
                    # x: (batch_size, input_dim) -> éœ€è¦è½¬æ¢ä¸º (batch_size, seq_len, input_dim)
                    # å¯¹äºé™æ€å‚æ•°é¢„æµ‹ï¼Œå°† seq_len è®¾ç½®ä¸º 1
                    x = x.unsqueeze(1)  # (batch_size, 1, input_dim)

                    # åˆå§‹åŒ–éšè—çŠ¶æ€
                    batch_size = x.size(0)
                    h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_dim,
                                   device=x.device)  # *2 for bidirectional

                    # GRUå‰å‘ä¼ æ’­
                    out, _ = self.gru(x, h0)  # out: (batch_size, seq_len, hidden_dim * 2)

                    # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
                    out = out[:, -1, :]  # (batch_size, hidden_dim * 2)

                    # æ‰¹å½’ä¸€åŒ–å’Œå…¨è¿æ¥å±‚
                    out = self.batch_norm(out)
                    out = self.dropout(self.relu(self.fc1(out)))
                    out = self.fc2(out)

                    return out

            return RNNModel(
                input_dim=self.input_dim,
                hidden_dim=128,
                output_dim=self.output_dim,
                num_layers=2
            ).to(self.device)

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")

    def train_model(self, model, X_train, y_train, X_val, y_val,
                   epochs=200, batch_size=64, lr=0.001):
        """
        è®­ç»ƒè´´ç‰‡å¤©çº¿è®¾è®¡æ¨¡å‹

        å‚æ•°:
        model: ç¥ç»ç½‘ç»œæ¨¡å‹
        X_train, y_train: è®­ç»ƒæ•°æ®
        X_val, y_val: éªŒè¯æ•°æ®
        epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹æ¬¡å¤§å°
        lr: å­¦ä¹ ç‡

        è¿”å›:
        è®­ç»ƒå†å²
        """
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataset = TensorDataset(X_train, y_train)
        val_dataset = TensorDataset(X_val, y_val)

        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

        # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5)

        # è®­ç»ƒå†å²
        history = {
            'train_loss': [],
            'val_loss': [],
            'train_rmse': [],
            'val_rmse': []
        }

        best_val_loss = float('inf')

        print("å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        for epoch in range(epochs):
            model.train()
            train_loss = 0.0

            for inputs, targets in train_loader:
                inputs, targets = inputs.to(self.device), targets.to(self.device)

                # å‰å‘ä¼ æ’­
                outputs = model(inputs)
                loss = criterion(outputs, targets)

                # åå‘ä¼ æ’­å’Œä¼˜åŒ–
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                train_loss += loss.item() * inputs.size(0)

            # è®¡ç®—å¹³å‡æŸå¤±
            train_loss /= len(train_loader.dataset)
            train_rmse = np.sqrt(train_loss)

            # éªŒè¯
            model.eval()
            val_loss = 0.0

            with torch.no_grad():
                for inputs, targets in val_loader:
                    inputs, targets = inputs.to(self.device), targets.to(self.device)
                    outputs = model(inputs)
                    loss = criterion(outputs, targets)
                    val_loss += loss.item() * inputs.size(0)

            val_loss /= len(val_loader.dataset)
            val_rmse = np.sqrt(val_loss)

            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step(val_loss)

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save(model.state_dict(), 'best_patch_antenna_model.pth')

            # è®°å½•å†å²
            history['train_loss'].append(train_loss)
            history['val_loss'].append(val_loss)
            history['train_rmse'].append(train_rmse)
            history['val_rmse'].append(val_rmse)

            # æ‰“å°è¿›åº¦
            if (epoch + 1) % 10 == 0:
                print(f"Epoch [{epoch+1}/{epochs}], "
                      f"Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}, "
                      f"Train RMSE: {train_rmse:.6f}, Val RMSE: {val_rmse:.6f}")

        print(f"è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
        return history

    def optimize_antenna(self, model, target_specs, param_bounds,
                        num_iterations=1000, learning_rate=0.01, device=None):
        """
        è´´ç‰‡å¤©çº¿å‚æ•°ä¼˜åŒ–ï¼ˆä¿®å¤RNNåå‘ä¼ æ’­é—®é¢˜ï¼‰

        å‚æ•°:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        target_specs: ç›®æ ‡æ€§èƒ½æŒ‡æ ‡ [S11æœ€å°å€¼, å¯¹åº”é¢‘ç‡, è¿œåŒºåœºå¢ç›Š]
        param_bounds: å‚æ•°è¾¹ç•Œ [[min1, max1], [min2, max2], [min3, max3], [min4, max4]]
        num_iterations: è¿­ä»£æ¬¡æ•°
        learning_rate: å­¦ä¹ ç‡
        device: è®¡ç®—è®¾å¤‡ï¼ˆGPU/CPUï¼‰
        """
        # 1. å¼ºåˆ¶æŒ‡å®šè®¾å¤‡ï¼ˆç¡®ä¿æ‰€æœ‰å¼ é‡åœ¨åŒä¸€è®¾å¤‡ï¼‰
        if device is None:
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ä¼˜åŒ–ä½¿ç”¨è®¾å¤‡: {device}")

        # 2. éªŒè¯è¾“å…¥
        if len(target_specs) != self.output_dim:
            raise ValueError(f"ç›®æ ‡æ€§èƒ½æŒ‡æ ‡åº”ä¸º {self.output_dim} ä¸ªï¼Œå®é™…ä¸º {len(target_specs)}")
        if param_bounds.shape != (self.input_dim, 2):
            raise ValueError(f"å‚æ•°è¾¹ç•Œåº”ä¸º {self.input_dim}x2 çš„æ•°ç»„ï¼Œå®é™…ä¸º {param_bounds.shape}")

        num_params = self.input_dim
        model = model.to(device)  # ç¡®ä¿æ¨¡å‹åœ¨ç›®æ ‡è®¾å¤‡

        # 3. åˆ›å»ºå¶å­å¼ é‡ï¼ˆç›´æ¥ç”¨randåˆ›å»ºï¼Œä¸ç»è¿‡å…¶ä»–è¿ç®—åŒ…è£¹ï¼‰
        params = torch.rand(num_params, dtype=torch.float32, device=device, requires_grad=True)
        print(f"è°ƒè¯•ï¼šåˆå§‹paramsæ˜¯å¦ä¸ºå¶å­å¼ é‡ = {params.is_leaf}")  # åº”è¾“å‡ºTrue

        # 4. å‚æ•°æ˜ å°„åˆ°æŒ‡å®šèŒƒå›´ï¼ˆç”¨in-placeæ“ä½œï¼Œä¸æ”¹å˜å¶å­å¼ é‡å±æ€§ï¼‰
        param_min = torch.tensor(param_bounds[:, 0], dtype=torch.float32, device=device)
        param_max = torch.tensor(param_bounds[:, 1], dtype=torch.float32, device=device)

        # ç”¨in-placeä¹˜æ³•å’ŒåŠ æ³•ï¼Œé¿å…åˆ›å»ºæ–°å¼ é‡ï¼ˆä¿æŒå¶å­å±æ€§ï¼‰
        params.data = params.data * (param_max - param_min) + param_min
        print(f"è°ƒè¯•ï¼šæ˜ å°„åparamsæ˜¯å¦ä¸ºå¶å­å¼ é‡ = {params.is_leaf}")  # åº”è¾“å‡ºTrue
        print(f"è°ƒè¯•ï¼šæ˜ å°„åparams.requires_grad = {params.requires_grad}")  # åº”è¾“å‡ºTrue

        # 5. ä¼˜åŒ–å™¨ï¼ˆç°åœ¨å¯ä»¥æ­£å¸¸ä¼˜åŒ–å¶å­å¼ é‡ï¼‰
        optimizer = optim.Adam([params], lr=learning_rate)
        target_tensor = torch.tensor(target_specs, dtype=torch.float32, device=device)

        best_loss = float('inf')
        best_params = None
        best_performance = None

        print("å¼€å§‹è´´ç‰‡å¤©çº¿å‚æ•°ä¼˜åŒ–...")
        print(f"ç›®æ ‡æ€§èƒ½: S11={target_specs[0]:.2f}dB, é¢‘ç‡={target_specs[1]:.2f}GHz, å¢ç›Š={target_specs[2]:.2f}dBi")

        # 6. å…³é”®ä¿®å¤ï¼šå¯¹äºRNNæ¨¡å‹ï¼Œä¸è¦è®¾ç½®ä¸ºevalæ¨¡å¼
        # å› ä¸ºCuDNN RNNåœ¨evalæ¨¡å¼ä¸‹ä¸èƒ½è¿›è¡Œåå‘ä¼ æ’­
        # æ›¿ä»£æ–¹æ¡ˆï¼šæ‰‹åŠ¨æ§åˆ¶dropoutå’Œbatch normçš„è¡Œä¸º
        model.train()  # ä¿æŒæ¨¡å‹åœ¨è®­ç»ƒæ¨¡å¼
        print(f"è°ƒè¯•ï¼šæ¨¡å‹æ¨¡å¼ = {'è®­ç»ƒ' if model.training else 'è¯„ä¼°'}")

        # ç¦ç”¨æ¨¡å‹å‚æ•°çš„æ¢¯åº¦è®¡ç®—ï¼ˆåªä¼˜åŒ–paramsï¼Œä¸æ›´æ–°æ¨¡å‹æƒé‡ï¼‰
        for p in model.parameters():
            p.requires_grad = False
            # å¯¹äºRNNå±‚ï¼Œç¡®ä¿åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ä¹Ÿèƒ½æ­£ç¡®å·¥ä½œ
            if isinstance(p, (nn.GRU, nn.LSTM, nn.RNN)):
                p.flatten_parameters()  # ä¼˜åŒ–å†…å­˜ä½¿ç”¨

        # ç‰¹æ®Šå¤„ç†ï¼šå¯¹äºbatch normå±‚ï¼Œå¼ºåˆ¶ä½¿ç”¨ç§»åŠ¨å¹³å‡
        # ç¡®ä¿åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ä¹Ÿèƒ½è·å¾—ç±»ä¼¼è¯„ä¼°æ¨¡å¼çš„ç¨³å®šç»“æœ
        def set_batch_norm_eval(model):
            for module in model.modules():
                if isinstance(module, nn.BatchNorm1d):
                    module.eval()

        # åœ¨æ¯æ¬¡å‰å‘ä¼ æ’­å‰è®¾ç½®batch normä¸ºevalæ¨¡å¼
        set_batch_norm_eval(model)

        for i in range(num_iterations):
            # 7. æ¸…é›¶æ¢¯åº¦
            optimizer.zero_grad()

            # 8. å½’ä¸€åŒ–å‚æ•°
            params_normalized = (params - param_min) / (param_max - param_min + 1e-8)
            params_normalized = torch.clamp(params_normalized, 0.0, 1.0)

            # 9. æ¨¡å‹é¢„æµ‹ï¼ˆå¯¹äºRNNï¼Œå¿…é¡»åœ¨è®­ç»ƒæ¨¡å¼ä¸‹è¿›è¡Œåå‘ä¼ æ’­ï¼‰
            performance = model(params_normalized.unsqueeze(0))[0]

            # 10. è®¡ç®—åŠ æƒæŸå¤±
            weights = torch.tensor([2.0, 1.0, 1.0], dtype=torch.float32, device=device)
            loss = torch.mean(weights * torch.square(performance - target_tensor))

            # 11. è°ƒè¯•ä¿¡æ¯
            if i == 0:
                print(f"è°ƒè¯•ï¼šperformance.requires_grad = {performance.requires_grad}")
                print(f"è°ƒè¯•ï¼šloss.requires_grad = {loss.requires_grad}")

            # 12. åå‘ä¼ æ’­ï¼ˆRNNå¿…é¡»åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼‰
            try:
                loss.backward()
            except RuntimeError as e:
                print(f"åå‘ä¼ æ’­é”™è¯¯: {e}")
                print("å°è¯•ç¦ç”¨CuDNNåŠ é€Ÿ...")
                # å¦‚æœCuDNNæœ‰é—®é¢˜ï¼Œå°è¯•ç¦ç”¨
                torch.backends.cudnn.enabled = False
                loss.backward()
                torch.backends.cudnn.enabled = True

            # 13. è°ƒè¯•æ¢¯åº¦
            if i == 0:
                if params.grad is not None:
                    print(f"è°ƒè¯•ï¼šæ¢¯åº¦è®¡ç®—æˆåŠŸï¼params.gradå½¢çŠ¶ = {params.grad.shape}")
                else:
                    print("è°ƒè¯•ï¼šè­¦å‘Šï¼params.gradä¸ºNone")

            # 14. æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_([params], max_norm=1.0)

            # 15. æ›´æ–°å‚æ•°
            optimizer.step()

            # 16. é™åˆ¶å‚æ•°è¾¹ç•Œ
            with torch.no_grad():
                params.clamp_(param_min, param_max)

            # 17. æ›´æ–°æœ€ä½³ç»“æœ
            current_loss = loss.item()
            if current_loss < best_loss:
                best_loss = current_loss
                best_params = params.detach().cpu().numpy().copy()
                best_performance = performance.detach().cpu().numpy().copy()

            # 18. æ‰“å°è¿›åº¦
            if (i + 1) % 100 == 0 or i == num_iterations - 1:
                print(f"Iteration {i + 1}/{num_iterations}, Loss: {current_loss:.6f}, "
                      f"Best Loss: {best_loss:.6f}, "
                      f"Current S11: {performance[0].item():.2f}dB")

        # 19. æ¢å¤æ¨¡å‹çŠ¶æ€
        model.train()
        for p in model.parameters():
            p.requires_grad = True

        # å¤„ç†ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False

        print(f"ä¼˜åŒ–ç»“æŸï¼æœ€ä½³æŸå¤±: {best_loss:.6f}")
        return best_params, best_performance, best_loss

    def visualize_results(self, history, y_true, y_pred):
        """
        å¯è§†åŒ–è®­ç»ƒç»“æœå’Œé¢„æµ‹æ€§èƒ½
        """
        # åˆ›å»ºå›¾å½¢ç›®å½•
        os.makedirs('patch_antenna_results', exist_ok=True)

        # 1. è®­ç»ƒæŸå¤±æ›²çº¿
        plt.figure(figsize=(12, 5))

        plt.subplot(1, 2, 1)
        plt.plot(history['train_loss'], label='è®­ç»ƒæŸå¤±')
        plt.plot(history['val_loss'], label='éªŒè¯æŸå¤±')
        plt.xlabel('Epoch')
        plt.ylabel('æŸå¤±')
        plt.title('è®­ç»ƒæŸå¤±æ›²çº¿')
        plt.legend()
        plt.grid(True)

        plt.subplot(1, 2, 2)
        plt.plot(history['train_rmse'], label='è®­ç»ƒRMSE')
        plt.plot(history['val_rmse'], label='éªŒè¯RMSE')
        plt.xlabel('Epoch')
        plt.ylabel('RMSE')
        plt.title('è®­ç»ƒRMSEæ›²çº¿')
        plt.legend()
        plt.grid(True)

        plt.tight_layout()
        plt.savefig('patch_antenna_results/training_curves.png', dpi=300, bbox_inches='tight')
        plt.close()

        # 2. é¢„æµ‹ vs çœŸå®å€¼æ•£ç‚¹å›¾
        fig, axes = plt.subplots(1, self.output_dim, figsize=(12, 4))

        for i in range(self.output_dim):
            ax = axes[i]
            ax.scatter(y_true[:, i], y_pred[:, i], alpha=0.6, s=10)

            # æ·»åŠ å¯¹è§’çº¿
            min_val = min(y_true[:, i].min(), y_pred[:, i].min())
            max_val = max(y_true[:, i].max(), y_pred[:, i].max())
            ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)

            ax.set_xlabel('çœŸå®å€¼')
            ax.set_ylabel('é¢„æµ‹å€¼')
            ax.set_title(self.perf_names[i])
            ax.grid(True)

            # è®¡ç®—RÂ²
            r2 = 1 - np.sum((y_true[:, i] - y_pred[:, i])**2) / np.sum((y_true[:, i] - y_true[:, i].mean())**2)
            ax.text(0.05, 0.95, f'RÂ² = {r2:.3f}', transform=ax.transAxes,
                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

        plt.tight_layout()
        plt.savefig('patch_antenna_results/prediction_scatter.png', dpi=300, bbox_inches='tight')
        plt.close()

        # 3. è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾
        errors = y_true - y_pred
        fig, axes = plt.subplots(1, self.output_dim, figsize=(12, 4))

        for i in range(self.output_dim):
            ax = axes[i]
            ax.hist(errors[:, i], bins=50, alpha=0.7, edgecolor='black')
            ax.set_xlabel('è¯¯å·®')
            ax.set_ylabel('é¢‘æ¬¡')
            ax.set_title(f'{self.perf_names[i]} è¯¯å·®åˆ†å¸ƒ')
            ax.grid(True)

            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            mean_err = np.mean(errors[:, i])
            std_err = np.std(errors[:, i])
            ax.axvline(mean_err, color='red', linestyle='--', label=f'å‡å€¼: {mean_err:.3f}')
            ax.legend()

        plt.tight_layout()
        plt.savefig('patch_antenna_results/error_distribution.png', dpi=300, bbox_inches='tight')
        plt.close()

        # 4. å‚æ•°ç›¸å…³æ€§åˆ†æ
        fig, axes = plt.subplots(self.output_dim, self.input_dim, figsize=(16, 12))

        for i in range(self.output_dim):
            for j in range(self.input_dim):
                ax = axes[i, j]
                ax.scatter(y_true[:, i], y_pred[:, i], alpha=0.6, s=5)
                ax.set_xlabel(self.perf_names[i])
                ax.set_ylabel(self.param_names[j])
                ax.grid(True)

        plt.tight_layout()
        plt.savefig('patch_antenna_results/correlation_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()

        print("å¯è§†åŒ–ç»“æœå·²ä¿å­˜åˆ° patch_antenna_results ç›®å½•")

    def hfss_interface(self, parameters):
        """
        HFSSä»¿çœŸæ¥å£

        å‚æ•°:
        parameters: å¤©çº¿å‚æ•° [è´´ç‰‡é•¿åº¦, è´´ç‰‡å®½åº¦, GNDåšåº¦, ä¿¡å·çº¿åšåº¦]

        è¿”å›:
        ä»¿çœŸå¾—åˆ°çš„æ€§èƒ½æŒ‡æ ‡
        """
        print("\n=== HFSSä»¿çœŸæ¥å£ ===")
        print("å¤©çº¿ç±»å‹: è´´ç‰‡å¤©çº¿")
        print(f"è®¾è®¡å‚æ•°: {parameters}")

        print("å‚æ•°è¯´æ˜:")
        for i, name in enumerate(self.param_names):
            print(f"  {name}: {parameters[i]:.3f}")

        print("\næ­£åœ¨è°ƒç”¨HFSSæ‰§è¡Œä»¥ä¸‹æ“ä½œ:")
        print("1. åˆ›å»ºæ–°çš„HFSSé¡¹ç›®")
        print("2. æ ¹æ®å‚æ•°å»ºç«‹è´´ç‰‡å¤©çº¿æ¨¡å‹")
        print("3. è®¾ç½®GNDç»“æ„å’Œä¿¡å·çº¿")
        print("4. è®¾ç½®ä»¿çœŸé¢‘ç‡èŒƒå›´å’Œè¾¹ç•Œæ¡ä»¶")
        print("5. è¿è¡Œç”µç£ä»¿çœŸ")
        print("6. æå–S11å‚æ•°å’Œè¿œåŒºåœºå¢ç›Š")

        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨HFSS API
        # è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„æ¨¡æ‹Ÿç»“æœ
        simulated_s11 = -20 - 0.1 * (parameters[0] + parameters[1]) + np.random.normal(0, 1)
        simulated_freq = 2.4 + 0.02 * parameters[0] + np.random.normal(0, 0.1)
        simulated_gain = 5.0 + 0.01 * (parameters[0] + parameters[1]) + np.random.normal(0, 0.2)

        simulated_performance = [simulated_s11, simulated_freq, simulated_gain]

        print(f"\nHFSSä»¿çœŸç»“æœ:")
        print(f"  S11æœ€å°å€¼: {simulated_performance[0]:.2f} dB")
        print(f"  å¯¹åº”é¢‘ç‡: {simulated_performance[1]:.2f} GHz")
        print(f"  è¿œåŒºåœºå¢ç›Š: {simulated_performance[2]:.2f} dBi")

        return simulated_performance

    def design_workflow(self, csv_file=None, param_cols=None, perf_cols=None,
                       model_type='resnet', epochs=200, use_synthetic_data=False):
        """
        å®Œæ•´çš„è´´ç‰‡å¤©çº¿è®¾è®¡å·¥ä½œæµç¨‹

        å‚æ•°:
        csv_file: CSVæ–‡ä»¶è·¯å¾„
        param_cols: å‚æ•°åˆ—ååˆ—è¡¨
        perf_cols: æ€§èƒ½åˆ—ååˆ—è¡¨
        model_type: æ¨¡å‹ç±»å‹
        epochs: è®­ç»ƒè½®æ•°
        use_synthetic_data: æ˜¯å¦ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œæµ‹è¯•

        è¿”å›:
        ä¼˜åŒ–åçš„å¤©çº¿è®¾è®¡ç»“æœ
        """
        print("=== è´´ç‰‡å¤©çº¿è®¾è®¡å·¥ä½œæµç¨‹ ===")
        print("=" * 60)
        start_time = time.time()

        # 1. åŠ è½½æ•°æ®
        print("\n1. åŠ è½½å¤©çº¿æ•°æ®...")
        if csv_file and not use_synthetic_data:
            X_scaled, y, X_original, y_original = self.load_csv_data(
                csv_file, param_cols, perf_cols
            )
        else:
            print("ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œæ¼”ç¤º")
            X_scaled, y, X_original, y_original = self.generate_synthetic_data(
                num_samples=5000
            )

        print(f"æ•°æ®é›†å¤§å°: {X_scaled.shape[0]} æ ·æœ¬")

        # 2. åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        X_train, X_val, y_train, y_val = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42
        )

        # 3. åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
        print(f"\n2. åˆ›å»º {model_type} æ¨¡å‹...")
        model = self.create_model(model_type)

        print("\n3. è®­ç»ƒæ¨¡å‹...")
        history = self.train_model(
            model, X_train, y_train, X_val, y_val,
            epochs=epochs, batch_size=64, lr=0.001
        )

        # 4. æ¨¡å‹è¯„ä¼°
        print("\n4. æ¨¡å‹æ€§èƒ½è¯„ä¼°...")
        model.eval()
        with torch.no_grad():
            y_pred_train = model(X_train).cpu().numpy()
            y_pred_val = model(X_val).cpu().numpy()

        # è®¡ç®—RÂ²åˆ†æ•°
        from sklearn.metrics import r2_score
        print("RÂ²å†³å®šç³»æ•° (è¶Šé«˜è¶Šå¥½):")
        for i, name in enumerate(self.perf_names):
            r2 = r2_score(y_val.cpu().numpy()[:, i], y_pred_val[:, i])
            print(f"  {name}: {r2:.4f}")

        # 5. å¯è§†åŒ–ç»“æœ
        print("\n5. ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
        self.visualize_results(history, y_val.cpu().numpy(), y_pred_val)

        # 6. å¤©çº¿å‚æ•°ä¼˜åŒ–
        print("\n6. è´´ç‰‡å¤©çº¿å‚æ•°ä¼˜åŒ–...")

        # å®šä¹‰è®¾è®¡ç›®æ ‡ (ç¤ºä¾‹ç›®æ ‡)
        target_specs = [
            -30.0,   # S11æœ€å°å€¼: -30dB (å°½å¯èƒ½å°)
            2.45,    # å¯¹åº”é¢‘ç‡: 2.45GHz (WiFié¢‘æ®µ)
            6.5      # è¿œåŒºåœºå¢ç›Š: 6.5dBi (é«˜å¢ç›Š)
        ]

        print(f"è®¾è®¡ç›®æ ‡:")
        for i, (name, target) in enumerate(zip(self.perf_names, target_specs)):
            print(f"  {name}: {target}")

        # å‚æ•°è¾¹ç•Œï¼ˆåŸºäºæ•°æ®èŒƒå›´ï¼‰
        param_min = X_original.min(axis=0)
        param_max = X_original.max(axis=0)
        param_bounds = np.column_stack([param_min, param_max])

        print(f"\nå‚æ•°ä¼˜åŒ–è¾¹ç•Œ:")
        for i, name in enumerate(self.param_names):
            print(f"  {name}: {param_bounds[i, 0]:.3f} - {param_bounds[i, 1]:.3f}")

        # æ‰§è¡Œä¼˜åŒ–
        optimal_params, predicted_performance, optimization_loss = self.optimize_antenna(
            model, target_specs, param_bounds, num_iterations=2000
        )

        print(f"\nä¼˜åŒ–ç»“æœ:")
        print(f"æœ€ä¼˜å‚æ•°:")
        for i, name in enumerate(self.param_names):
            print(f"  {name}: {optimal_params[i]:.3f}")

        print(f"\né¢„æµ‹æ€§èƒ½:")
        for i, name in enumerate(self.perf_names):
            diff = abs(predicted_performance[i] - target_specs[i])
            status = "âœ“" if (name == self.perf_names[0] and predicted_performance[i] <= target_specs[i]) or \
                           (name == self.perf_names[1] and abs(diff) < 0.1) or \
                           (name == self.perf_names[2] and predicted_performance[i] >= target_specs[i]) else "âš ï¸"
            print(f"  {status} {name}: {predicted_performance[i]:.3f} (ç›®æ ‡: {target_specs[i]})")

        # 7. HFSSä»¿çœŸéªŒè¯
        print(f"\n7. HFSSä»¿çœŸéªŒè¯...")
        simulated_performance = self.hfss_interface(optimal_params)

        # 8. è®¾è®¡å¯è¡Œæ€§åˆ†æ
        print(f"\n8. è®¾è®¡å¯è¡Œæ€§åˆ†æ:")
        is_feasible = True

        # S11æ£€æŸ¥
        if predicted_performance[0] > -15:  # S11 > -15dB è¢«è®¤ä¸ºæ€§èƒ½è¾ƒå·®
            print(f"  âš ï¸  S11å€¼ {predicted_performance[0]:.2f}dB åé«˜ï¼Œå¯èƒ½éœ€è¦æ”¹è¿›åŒ¹é…")
            is_feasible = False
        else:
            print(f"  âœ“ S11å€¼ {predicted_performance[0]:.2f}dB æ»¡è¶³è¦æ±‚")

        # é¢‘ç‡æ£€æŸ¥
        if not (2.4 <= predicted_performance[1] <= 2.5):  # WiFi 2.4GHzé¢‘æ®µ
            print(f"  âš ï¸  å·¥ä½œé¢‘ç‡ {predicted_performance[1]:.2f}GHz ä¸åœ¨WiFi 2.4GHzé¢‘æ®µå†…")
            is_feasible = False
        else:
            print(f"  âœ“ å·¥ä½œé¢‘ç‡åœ¨WiFi 2.4GHzé¢‘æ®µå†…")

        # å¢ç›Šæ£€æŸ¥
        if predicted_performance[2] < 5.0:  # å¢ç›Šä½äº5dBi
            print(f"  âš ï¸  å¢ç›Š {predicted_performance[2]:.2f}dBi åä½")
            is_feasible = False
        else:
            print(f"  âœ“ å¢ç›Š {predicted_performance[2]:.2f}dBi æ»¡è¶³è¦æ±‚")

        end_time = time.time()
        print(f"\n=== è´´ç‰‡å¤©çº¿è®¾è®¡å·¥ä½œæµç¨‹å®Œæˆ ===")
        print(f"æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")

        if is_feasible:
            print("ğŸ‰ è®¾è®¡æˆåŠŸï¼è¯¥è´´ç‰‡å¤©çº¿è®¾è®¡æ»¡è¶³è¦æ±‚ã€‚")
        else:
            print("âš ï¸  è®¾è®¡åŸºæœ¬å®Œæˆï¼Œä½†éƒ¨åˆ†æŒ‡æ ‡éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚")

        # ä¿å­˜è®¾è®¡ç»“æœ
        design_result = {
            'optimal_parameters': optimal_params,
            'predicted_performance': predicted_performance,
            'simulated_performance': simulated_performance,
            'target_specifications': target_specs,
            'optimization_loss': optimization_loss,
            'model_type': model_type,
            'training_history': history,
            'is_feasible': is_feasible,
            'total_time': end_time - start_time
        }

        np.save('patch_antenna_results/design_result.npy', design_result)
        print("è®¾è®¡ç»“æœå·²ä¿å­˜åˆ° patch_antenna_results/design_result.npy")

        return design_result

if __name__ == "__main__":
    # æ¼”ç¤ºä½¿ç”¨
    system = PatchAntennaDesignSystem()

    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    import sys
    if len(sys.argv) > 1 and sys.argv[1].endswith('.csv'):
        csv_file = sys.argv[1]
        print(f"ä½¿ç”¨CSVæ–‡ä»¶: {csv_file}")

        # å¦‚æœæŒ‡å®šäº†åˆ—å
        param_cols = None
        perf_cols = None
        if len(sys.argv) > 3:
            param_cols = sys.argv[2].split(',')
            perf_cols = sys.argv[3].split(',')

        # æ‰§è¡Œè®¾è®¡æµç¨‹
        result = system.design_workflow(
            csv_file=csv_file,
            param_cols=param_cols,
            perf_cols=perf_cols,
            model_type='resnet',
            epochs=200
        )
    else:
        # ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œæ¼”ç¤º
        print("ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œæ¼”ç¤º (æ·»åŠ CSVæ–‡ä»¶è·¯å¾„ä½œä¸ºå‚æ•°å¯ä½¿ç”¨çœŸå®æ•°æ®)")
        result = system.design_workflow(
            model_type='resnet',
            epochs=200,
            use_synthetic_data=True
        )

    print("\nè®¾è®¡æµç¨‹å…¨éƒ¨å®Œæˆï¼")