"""
åŸºäºPyTorchçš„è´´ç‰‡å¤©çº¿è®¾è®¡ç³»ç»Ÿ
ä¸“é—¨é’ˆå¯¹:
- è¾“å…¥: è´´ç‰‡é•¿å®½ã€GNDåšåº¦ã€ä¿¡å·çº¿åšåº¦
- è¾“å‡º: S11æœ€å°å€¼ã€å¯¹åº”é¢‘ç‡ã€è¿œåŒºåœºå¢ç›Š
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from torch.utils.data import Dataset, DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from python_hfss import *
import time
import os

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class PatchAntennaDesignSystem:
    def __init__(self, device=None):
        """åˆå§‹åŒ–è´´ç‰‡å¤©çº¿è®¾è®¡ç³»ç»Ÿ"""
        self.device = device if device else torch.device(
            'cuda' if torch.cuda.is_available() else 'cpu'
        )
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")

        # ç³»ç»Ÿå‚æ•°
        self.scaler = StandardScaler()
        self.input_dim = 4  # è´´ç‰‡é•¿å®½ã€GNDåšåº¦ã€ä¿¡å·çº¿åšåº¦
        self.output_dim = 3  # S11æœ€å°å€¼ã€å¯¹åº”é¢‘ç‡ã€è¿œåŒºåœºå¢ç›Š

        # å‚æ•°å’Œæ€§èƒ½æŒ‡æ ‡åç§°
        self.param_names = ['è´´ç‰‡é•¿åº¦(mm)', 'è´´ç‰‡å®½åº¦(mm)', 'GNDåšåº¦(mm)', 'ä¿¡å·çº¿åšåº¦(mm)']
        self.perf_names = ['S11æœ€å°å€¼(dB)', 'å¯¹åº”é¢‘ç‡(GHz)', 'è¿œåŒºåœºå¢ç›Š(dBi)']

    def load_csv_data(self, csv_file, param_cols=None, perf_cols=None):
        """
        ä»CSVæ–‡ä»¶åŠ è½½è´´ç‰‡å¤©çº¿æ•°æ®

        å‚æ•°:
        csv_file: CSVæ–‡ä»¶è·¯å¾„
        param_cols: å‚æ•°åˆ—ååˆ—è¡¨ (å¯é€‰)
        perf_cols: æ€§èƒ½æŒ‡æ ‡åˆ—ååˆ—è¡¨ (å¯é€‰)

        è¿”å›:
        X_scaled: å½’ä¸€åŒ–çš„å¤©çº¿å‚æ•°
        y: å¤©çº¿æ€§èƒ½æŒ‡æ ‡
        X_original: åŸå§‹å¤©çº¿å‚æ•°
        y_original: åŸå§‹æ€§èƒ½æŒ‡æ ‡
        """
        print(f"ä»CSVæ–‡ä»¶åŠ è½½æ•°æ®: {csv_file}")

        # è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv(csv_file)
        print(f"æ•°æ®å½¢çŠ¶: {df.shape}")
        print(f"åˆ—å: {list(df.columns)}")

        # å¦‚æœæ²¡æœ‰æŒ‡å®šåˆ—åï¼Œåˆ™ä½¿ç”¨é»˜è®¤åˆ—å
        if param_cols is None:
            param_cols = ['patch_length', 'patch_width', 'gnd_thickness', 'signal_thickness']
            print(f"ä½¿ç”¨é»˜è®¤å‚æ•°åˆ—å: {param_cols}")

        if perf_cols is None:
            perf_cols = ['s11_min', 'freq_at_s11_min', 'far_field_gain']
            print(f"ä½¿ç”¨é»˜è®¤æ€§èƒ½åˆ—å: {perf_cols}")

        # éªŒè¯åˆ—åæ˜¯å¦å­˜åœ¨
        for col in param_cols + perf_cols:
            if col not in df.columns:
                raise ValueError(f"åˆ—å '{col}' ä¸åœ¨CSVæ–‡ä»¶ä¸­")

        # æå–æ•°æ®
        X_original = df[param_cols].values
        y_original = df[perf_cols].values

        # éªŒè¯æ•°æ®ç»´åº¦
        if X_original.shape[1] != self.input_dim:
            raise ValueError(f"å‚æ•°åˆ—æ•°åº”ä¸º {self.input_dim}ï¼Œä½†å®é™…ä¸º {X_original.shape[1]}")

        if y_original.shape[1] != self.output_dim:
            raise ValueError(f"æ€§èƒ½åˆ—æ•°åº”ä¸º {self.output_dim}ï¼Œä½†å®é™…ä¸º {y_original.shape[1]}")

        # æ•°æ®å½’ä¸€åŒ–
        X_scaled = self.scaler.fit_transform(X_original)

        print(f"å‚æ•°æ•°æ®å½¢çŠ¶: {X_original.shape}")
        print(f"æ€§èƒ½æ•°æ®å½¢çŠ¶: {y_original.shape}")

        # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡ä¿¡æ¯
        print(f"\nå‚æ•°ç»Ÿè®¡:")
        for i, (name, col) in enumerate(zip(self.param_names, param_cols)):
            print(f"  {name}: å‡å€¼={X_original[:, i].mean():.3f}, æ ‡å‡†å·®={X_original[:, i].std():.3f}")

        print(f"\næ€§èƒ½æŒ‡æ ‡ç»Ÿè®¡:")
        for i, (name, col) in enumerate(zip(self.perf_names, perf_cols)):
            print(f"  {name}: å‡å€¼={y_original[:, i].mean():.3f}, æ ‡å‡†å·®={y_original[:, i].std():.3f}")

        return (torch.tensor(X_scaled, dtype=torch.float32),
                torch.tensor(y_original, dtype=torch.float32),
                X_original, y_original)

    def generate_synthetic_data(self, num_samples=5000):
        """
        ç”Ÿæˆåˆæˆçš„è´´ç‰‡å¤©çº¿æ•°æ®ç”¨äºæµ‹è¯•

        å‚æ•°:
        num_samples: æ ·æœ¬æ•°é‡

        è¿”å›:
        X_scaled: å½’ä¸€åŒ–çš„å¤©çº¿å‚æ•°
        y: å¤©çº¿æ€§èƒ½æŒ‡æ ‡
        X_original: åŸå§‹å¤©çº¿å‚æ•°
        y_original: åŸå§‹æ€§èƒ½æŒ‡æ ‡
        """
        np.random.seed(42)
        print(f"ç”Ÿæˆåˆæˆè´´ç‰‡å¤©çº¿æ•°æ®ï¼Œæ ·æœ¬æ•°: {num_samples}")

        # è´´ç‰‡å¤©çº¿å‚æ•°èŒƒå›´
        patch_length = np.random.uniform(5, 15, num_samples)  # è´´ç‰‡é•¿åº¦ 10-50mm
        patch_width = np.random.uniform(5, 15, num_samples)   # è´´ç‰‡å®½åº¦ 10-50mm
        gnd_thickness = np.random.uniform(0.01, 0.05, num_samples)  # GNDåšåº¦ 0.5-3.0mm
        signal_thickness = np.random.uniform(0.01, 0.05, num_samples)  # ä¿¡å·çº¿åšåº¦ 0.1-1.0mm

        X_original = np.column_stack([patch_length, patch_width, gnd_thickness, signal_thickness])

        # æ€§èƒ½æŒ‡æ ‡è®¡ç®—ï¼ˆåŸºäºç”µç£å­¦åŸç†çš„ç®€åŒ–æ¨¡å‹ï¼‰
        c = 3e8  # å…‰é€Ÿ

        # è°æŒ¯é¢‘ç‡è®¡ç®—
        L_meters = patch_length * 1e-3
        freq = c / (2 * L_meters * np.sqrt(4.4)) / 1e9  # å‡è®¾ä»‹ç”µå¸¸æ•°ä¸º4.4 (FR4)
        freq += np.random.normal(0, 0.2, num_samples)  # æ·»åŠ å™ªå£°

        # S11æœ€å°å€¼è®¡ç®— (ä¸å¤©çº¿å°ºå¯¸å’ŒåŒ¹é…æœ‰å…³)
        s11_min = -25 - 0.1 * (patch_length + patch_width) + np.random.normal(0, 2, num_samples)
        s11_min = np.clip(s11_min, -40, -10)  # S11èŒƒå›´é™åˆ¶åœ¨-40åˆ°-10dB

        # è¿œåŒºåœºå¢ç›Šè®¡ç®—
        gain = 2.0 + 0.02 * (patch_length + patch_width) - 0.5 * gnd_thickness + np.random.normal(0, 0.3, num_samples)
        gain = np.clip(gain, 0, 10)  # å¢ç›ŠèŒƒå›´é™åˆ¶åœ¨0-10dBi

        y_original = np.column_stack([s11_min, freq, gain])

        # æ•°æ®å½’ä¸€åŒ–
        X_scaled = self.scaler.fit_transform(X_original)

        print(f"åˆæˆæ•°æ®ç”Ÿæˆå®Œæˆ")
        print(f"å‚æ•°æ•°æ®å½¢çŠ¶: {X_original.shape}")
        print(f"æ€§èƒ½æ•°æ®å½¢çŠ¶: {y_original.shape}")

        return (torch.tensor(X_scaled, dtype=torch.float32),
                torch.tensor(y_original, dtype=torch.float32),
                X_original, y_original)

    def create_model(self, model_type='resnet'):
        """
        åˆ›å»ºè´´ç‰‡å¤©çº¿è®¾è®¡ç¥ç»ç½‘ç»œæ¨¡å‹

        å‚æ•°:
        model_type: æ¨¡å‹ç±»å‹ ('mlp', 'resnet', 'cnn')

        è¿”å›:
        ç¥ç»ç½‘ç»œæ¨¡å‹
        """
        if model_type == 'mlp':
            return nn.Sequential(
                nn.Linear(self.input_dim, 128),
                nn.BatchNorm1d(128),
                nn.ReLU(),
                nn.Dropout(0.2),

                nn.Linear(128, 256),
                nn.BatchNorm1d(256),
                nn.ReLU(),
                nn.Dropout(0.3),

                nn.Linear(256, 128),
                nn.BatchNorm1d(128),
                nn.ReLU(),
                nn.Dropout(0.2),

                nn.Linear(128, self.output_dim)
            ).to(self.device)

        elif model_type == 'resnet':
            # æ®‹å·®ç½‘ç»œ
            class ResidualBlock(nn.Module):
                def __init__(self, dim):
                    super().__init__()
                    self.block = nn.Sequential(
                        nn.Linear(dim, dim),
                        nn.BatchNorm1d(dim),
                        nn.ReLU(),
                        nn.Linear(dim, dim),
                        nn.BatchNorm1d(dim)
                    )
                    self.relu = nn.ReLU()

                def forward(self, x):
                    return self.relu(x + self.block(x))

            return nn.Sequential(
                nn.Linear(self.input_dim, 128),
                nn.BatchNorm1d(128),
                nn.ReLU(),

                ResidualBlock(128),
                ResidualBlock(128),

                nn.Linear(128, 256),
                nn.BatchNorm1d(256),
                nn.ReLU(),

                ResidualBlock(256),
                ResidualBlock(256),

                nn.Linear(256, self.output_dim)
            ).to(self.device)

        elif model_type == 'cnn':
            # ä¸€ç»´å·ç§¯ç½‘ç»œ
            return nn.Sequential(
                nn.Unflatten(1, (1, self.input_dim)),
                nn.Conv1d(1, 32, kernel_size=3, padding=1),
                nn.BatchNorm1d(32),
                nn.ReLU(),
                nn.MaxPool1d(2),

                nn.Conv1d(32, 64, kernel_size=3, padding=1),
                nn.BatchNorm1d(64),
                nn.ReLU(),
                nn.MaxPool1d(2),

                nn.Flatten(),
                nn.Linear(64 * (self.input_dim // 4), 128),
                nn.ReLU(),
                nn.Linear(128, self.output_dim)
            ).to(self.device)

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")

    def train_model(self, model, X_train, y_train, X_val, y_val,
                   epochs=200, batch_size=64, lr=0.001):
        """
        è®­ç»ƒè´´ç‰‡å¤©çº¿è®¾è®¡æ¨¡å‹

        å‚æ•°:
        model: ç¥ç»ç½‘ç»œæ¨¡å‹
        X_train, y_train: è®­ç»ƒæ•°æ®
        X_val, y_val: éªŒè¯æ•°æ®
        epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹æ¬¡å¤§å°
        lr: å­¦ä¹ ç‡

        è¿”å›:
        è®­ç»ƒå†å²
        """
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataset = TensorDataset(X_train, y_train)
        val_dataset = TensorDataset(X_val, y_val)

        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

        # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5)

        # è®­ç»ƒå†å²
        history = {
            'train_loss': [],
            'val_loss': [],
            'train_rmse': [],
            'val_rmse': []
        }

        best_val_loss = float('inf')

        print("å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        for epoch in range(epochs):
            model.train()
            train_loss = 0.0

            for inputs, targets in train_loader:
                inputs, targets = inputs.to(self.device), targets.to(self.device)

                # å‰å‘ä¼ æ’­
                outputs = model(inputs)
                loss = criterion(outputs, targets)

                # åå‘ä¼ æ’­å’Œä¼˜åŒ–
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                train_loss += loss.item() * inputs.size(0)

            # è®¡ç®—å¹³å‡æŸå¤±
            train_loss /= len(train_loader.dataset)
            train_rmse = np.sqrt(train_loss)

            # éªŒè¯
            model.eval()
            val_loss = 0.0

            with torch.no_grad():
                for inputs, targets in val_loader:
                    inputs, targets = inputs.to(self.device), targets.to(self.device)
                    outputs = model(inputs)
                    loss = criterion(outputs, targets)
                    val_loss += loss.item() * inputs.size(0)

            val_loss /= len(val_loader.dataset)
            val_rmse = np.sqrt(val_loss)

            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step(val_loss)

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save(model.state_dict(), 'best_patch_antenna_model.pth')

            # è®°å½•å†å²
            history['train_loss'].append(train_loss)
            history['val_loss'].append(val_loss)
            history['train_rmse'].append(train_rmse)
            history['val_rmse'].append(val_rmse)

            # æ‰“å°è¿›åº¦
            if (epoch + 1) % 10 == 0:
                print(f"Epoch [{epoch+1}/{epochs}], "
                      f"Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}, "
                      f"Train RMSE: {train_rmse:.6f}, Val RMSE: {val_rmse:.6f}")

        print(f"è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
        return history

    def optimize_antenna(self, model, target_specs, param_bounds,
                        num_iterations=1000, learning_rate=0.01):
        """
        è´´ç‰‡å¤©çº¿å‚æ•°ä¼˜åŒ–

        å‚æ•°:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        target_specs: ç›®æ ‡æ€§èƒ½æŒ‡æ ‡ [S11æœ€å°å€¼, å¯¹åº”é¢‘ç‡, è¿œåŒºåœºå¢ç›Š]
        param_bounds: å‚æ•°è¾¹ç•Œ [[min1, max1], [min2, max2], [min3, max3], [min4, max4]]
        num_iterations: è¿­ä»£æ¬¡æ•°
        learning_rate: å­¦ä¹ ç‡

        è¿”å›:
        ä¼˜åŒ–åçš„å‚æ•°å’Œé¢„æµ‹æ€§èƒ½
        """
        # éªŒè¯è¾“å…¥
        if len(target_specs) != self.output_dim:
            raise ValueError(f"ç›®æ ‡æ€§èƒ½æŒ‡æ ‡åº”ä¸º {self.output_dim} ä¸ªï¼Œå®é™…ä¸º {len(target_specs)}")

        if param_bounds.shape != (self.input_dim, 2):
            raise ValueError(f"å‚æ•°è¾¹ç•Œåº”ä¸º {self.input_dim}x2 çš„æ•°ç»„")

        num_params = self.input_dim

        # åˆå§‹åŒ–å‚æ•°
        params = torch.rand(num_params, requires_grad=True, device=self.device, dtype=torch.float32)

        # å°†å‚æ•°æ˜ å°„åˆ°æŒ‡å®šèŒƒå›´
        param_bounds_tensor = torch.tensor(param_bounds, dtype=torch.float32, device=self.device)
        for i in range(num_params):
            params.data[i] = param_bounds_tensor[i, 0] + params.data[i] * (param_bounds_tensor[i, 1] - param_bounds_tensor[i, 0])

        optimizer = optim.Adam([params], lr=learning_rate)
        target_tensor = torch.tensor(target_specs, dtype=torch.float32, device=self.device)

        best_loss = float('inf')
        best_params = None
        best_performance = None

        print("å¼€å§‹è´´ç‰‡å¤©çº¿å‚æ•°ä¼˜åŒ–...")
        print(f"ç›®æ ‡æ€§èƒ½: S11={target_specs[0]:.2f}dB, é¢‘ç‡={target_specs[1]:.2f}GHz, å¢ç›Š={target_specs[2]:.2f}dBi")

        for i in range(num_iterations):
            # å½’ä¸€åŒ–å‚æ•°
            params_normalized = (params - param_bounds_tensor[:, 0]) / \
                              (param_bounds_tensor[:, 1] - param_bounds_tensor[:, 0])

            # é¢„æµ‹æ€§èƒ½
            performance = model(params_normalized.unsqueeze(0))[0]

            # è®¡ç®—æŸå¤± (åŠ æƒæŸå¤±ï¼Œæ›´å…³æ³¨é‡è¦çš„æ€§èƒ½æŒ‡æ ‡)
            # S11æƒé‡æ›´é«˜ï¼Œå› ä¸ºå®ƒå¯¹å¤©çº¿æ€§èƒ½å½±å“æ›´å¤§
            weights = torch.tensor([2.0, 1.0, 1.0], device=self.device)  # S11æƒé‡æ›´é«˜
            loss = torch.mean(weights * torch.square(performance - target_tensor))

            # åå‘ä¼ æ’­å’Œä¼˜åŒ–
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # é™åˆ¶å‚æ•°åœ¨è¾¹ç•Œå†…
            for j in range(num_params):
                params.data[j] = torch.clamp(params.data[j], param_bounds_tensor[j, 0], param_bounds_tensor[j, 1])

            # æ›´æ–°æœ€ä½³ç»“æœ
            if loss.item() < best_loss:
                best_loss = loss.item()
                best_params = params.detach().cpu().numpy().copy()
                best_performance = performance.detach().cpu().numpy().copy()

            if (i + 1) % 100 == 0:
                print(f"Iteration {i+1}/{num_iterations}, Loss: {loss.item():.6f}, "
                      f"Best Loss: {best_loss:.6f}")

        return best_params, best_performance, best_loss

    def visualize_results(self, history, y_true, y_pred):
        """
        å¯è§†åŒ–è®­ç»ƒç»“æœå’Œé¢„æµ‹æ€§èƒ½
        """
        # åˆ›å»ºå›¾å½¢ç›®å½•
        os.makedirs('patch_antenna_results', exist_ok=True)

        # 1. è®­ç»ƒæŸå¤±æ›²çº¿
        plt.figure(figsize=(12, 5))

        plt.subplot(1, 2, 1)
        plt.plot(history['train_loss'], label='è®­ç»ƒæŸå¤±')
        plt.plot(history['val_loss'], label='éªŒè¯æŸå¤±')
        plt.xlabel('Epoch')
        plt.ylabel('æŸå¤±')
        plt.title('è®­ç»ƒæŸå¤±æ›²çº¿')
        plt.legend()
        plt.grid(True)

        plt.subplot(1, 2, 2)
        plt.plot(history['train_rmse'], label='è®­ç»ƒRMSE')
        plt.plot(history['val_rmse'], label='éªŒè¯RMSE')
        plt.xlabel('Epoch')
        plt.ylabel('RMSE')
        plt.title('è®­ç»ƒRMSEæ›²çº¿')
        plt.legend()
        plt.grid(True)

        plt.tight_layout()
        plt.savefig('patch_antenna_results/training_curves.png', dpi=300, bbox_inches='tight')
        plt.close()

        # 2. é¢„æµ‹ vs çœŸå®å€¼æ•£ç‚¹å›¾
        fig, axes = plt.subplots(1, self.output_dim, figsize=(12, 4))

        for i in range(self.output_dim):
            ax = axes[i]
            ax.scatter(y_true[:, i], y_pred[:, i], alpha=0.6, s=10)

            # æ·»åŠ å¯¹è§’çº¿
            min_val = min(y_true[:, i].min(), y_pred[:, i].min())
            max_val = max(y_true[:, i].max(), y_pred[:, i].max())
            ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)

            ax.set_xlabel('çœŸå®å€¼')
            ax.set_ylabel('é¢„æµ‹å€¼')
            ax.set_title(self.perf_names[i])
            ax.grid(True)

            # è®¡ç®—RÂ²
            r2 = 1 - np.sum((y_true[:, i] - y_pred[:, i])**2) / np.sum((y_true[:, i] - y_true[:, i].mean())**2)
            ax.text(0.05, 0.95, f'RÂ² = {r2:.3f}', transform=ax.transAxes,
                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

        plt.tight_layout()
        plt.savefig('patch_antenna_results/prediction_scatter.png', dpi=300, bbox_inches='tight')
        plt.close()

        # 3. è¯¯å·®åˆ†å¸ƒç›´æ–¹å›¾
        errors = y_true - y_pred
        fig, axes = plt.subplots(1, self.output_dim, figsize=(12, 4))

        for i in range(self.output_dim):
            ax = axes[i]
            ax.hist(errors[:, i], bins=50, alpha=0.7, edgecolor='black')
            ax.set_xlabel('è¯¯å·®')
            ax.set_ylabel('é¢‘æ¬¡')
            ax.set_title(f'{self.perf_names[i]} è¯¯å·®åˆ†å¸ƒ')
            ax.grid(True)

            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            mean_err = np.mean(errors[:, i])
            std_err = np.std(errors[:, i])
            ax.axvline(mean_err, color='red', linestyle='--', label=f'å‡å€¼: {mean_err:.3f}')
            ax.legend()

        plt.tight_layout()
        plt.savefig('patch_antenna_results/error_distribution.png', dpi=300, bbox_inches='tight')
        plt.close()

        # 4. å‚æ•°ç›¸å…³æ€§åˆ†æ
        fig, axes = plt.subplots(self.output_dim, self.input_dim, figsize=(16, 12))

        for i in range(self.output_dim):
            for j in range(self.input_dim):
                ax = axes[i, j]
                ax.scatter(y_true[:, i], y_pred[:, i], alpha=0.6, s=5)
                ax.set_xlabel(self.perf_names[i])
                ax.set_ylabel(self.param_names[j])
                ax.grid(True)

        plt.tight_layout()
        plt.savefig('patch_antenna_results/correlation_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()

        print("å¯è§†åŒ–ç»“æœå·²ä¿å­˜åˆ° patch_antenna_results ç›®å½•")

    def hfss_interface(self, parameters):
        """
        HFSSä»¿çœŸæ¥å£

        å‚æ•°:
        parameters: å¤©çº¿å‚æ•° [è´´ç‰‡é•¿åº¦, è´´ç‰‡å®½åº¦, GNDåšåº¦, ä¿¡å·çº¿åšåº¦]

        è¿”å›:
        ä»¿çœŸå¾—åˆ°çš„æ€§èƒ½æŒ‡æ ‡
        """
        print("\n=== HFSSä»¿çœŸæ¥å£ ===")
        print("å¤©çº¿ç±»å‹: è´´ç‰‡å¤©çº¿")
        print(f"è®¾è®¡å‚æ•°: {parameters}")

        print("å‚æ•°è¯´æ˜:")
        for i, name in enumerate(self.param_names):
            print(f"  {name}: {parameters[i]:.3f}")

        print("\næ­£åœ¨è°ƒç”¨HFSSæ‰§è¡Œä»¥ä¸‹æ“ä½œ:")
        print("1. åˆ›å»ºæ–°çš„HFSSé¡¹ç›®")
        print("2. æ ¹æ®å‚æ•°å»ºç«‹è´´ç‰‡å¤©çº¿æ¨¡å‹")
        print("3. è®¾ç½®GNDç»“æ„å’Œä¿¡å·çº¿")
        print("4. è®¾ç½®ä»¿çœŸé¢‘ç‡èŒƒå›´å’Œè¾¹ç•Œæ¡ä»¶")
        print("5. è¿è¡Œç”µç£ä»¿çœŸ")
        print("6. æå–S11å‚æ•°å’Œè¿œåŒºåœºå¢ç›Š")

        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨HFSS API
        # è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„æ¨¡æ‹Ÿç»“æœ
        simulated_s11 = -20 - 0.1 * (parameters[0] + parameters[1]) + np.random.normal(0, 1)
        simulated_freq = 2.4 + 0.02 * parameters[0] + np.random.normal(0, 0.1)
        simulated_gain = 5.0 + 0.01 * (parameters[0] + parameters[1]) + np.random.normal(0, 0.2)

        simulated_performance = [simulated_s11, simulated_freq, simulated_gain]

        print(f"\nHFSSä»¿çœŸç»“æœ:")
        print(f"  S11æœ€å°å€¼: {simulated_performance[0]:.2f} dB")
        print(f"  å¯¹åº”é¢‘ç‡: {simulated_performance[1]:.2f} GHz")
        print(f"  è¿œåŒºåœºå¢ç›Š: {simulated_performance[2]:.2f} dBi")

        return simulated_performance

    def design_workflow(self, csv_file=None, param_cols=None, perf_cols=None,
                       model_type='resnet', epochs=200, use_synthetic_data=False):
        """
        å®Œæ•´çš„è´´ç‰‡å¤©çº¿è®¾è®¡å·¥ä½œæµç¨‹

        å‚æ•°:
        csv_file: CSVæ–‡ä»¶è·¯å¾„
        param_cols: å‚æ•°åˆ—ååˆ—è¡¨
        perf_cols: æ€§èƒ½åˆ—ååˆ—è¡¨
        model_type: æ¨¡å‹ç±»å‹
        epochs: è®­ç»ƒè½®æ•°
        use_synthetic_data: æ˜¯å¦ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œæµ‹è¯•

        è¿”å›:
        ä¼˜åŒ–åçš„å¤©çº¿è®¾è®¡ç»“æœ
        """
        print("=== è´´ç‰‡å¤©çº¿è®¾è®¡å·¥ä½œæµç¨‹ ===")
        print("=" * 60)
        start_time = time.time()

        # 1. åŠ è½½æ•°æ®
        print("\n1. åŠ è½½å¤©çº¿æ•°æ®...")
        if csv_file and not use_synthetic_data:
            X_scaled, y, X_original, y_original = self.load_csv_data(
                csv_file, param_cols, perf_cols
            )
        else:
            print("ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œæ¼”ç¤º")
            X_scaled, y, X_original, y_original = self.generate_synthetic_data(
                num_samples=5000
            )

        print(f"æ•°æ®é›†å¤§å°: {X_scaled.shape[0]} æ ·æœ¬")

        # 2. åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        X_train, X_val, y_train, y_val = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42
        )

        # 3. åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
        print(f"\n2. åˆ›å»º {model_type} æ¨¡å‹...")
        model = self.create_model(model_type)

        print("\n3. è®­ç»ƒæ¨¡å‹...")
        history = self.train_model(
            model, X_train, y_train, X_val, y_val,
            epochs=epochs, batch_size=64, lr=0.001
        )

        # 4. æ¨¡å‹è¯„ä¼°
        print("\n4. æ¨¡å‹æ€§èƒ½è¯„ä¼°...")
        model.eval()
        with torch.no_grad():
            y_pred_train = model(X_train).cpu().numpy()
            y_pred_val = model(X_val).cpu().numpy()

        # è®¡ç®—RÂ²åˆ†æ•°
        from sklearn.metrics import r2_score
        print("RÂ²å†³å®šç³»æ•° (è¶Šé«˜è¶Šå¥½):")
        for i, name in enumerate(self.perf_names):
            r2 = r2_score(y_val.cpu().numpy()[:, i], y_pred_val[:, i])
            print(f"  {name}: {r2:.4f}")

        # 5. å¯è§†åŒ–ç»“æœ
        print("\n5. ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
        self.visualize_results(history, y_val.cpu().numpy(), y_pred_val)

        # 6. å¤©çº¿å‚æ•°ä¼˜åŒ–
        print("\n6. è´´ç‰‡å¤©çº¿å‚æ•°ä¼˜åŒ–...")

        # å®šä¹‰è®¾è®¡ç›®æ ‡ (ç¤ºä¾‹ç›®æ ‡)
        target_specs = [
            -30.0,   # S11æœ€å°å€¼: -30dB (å°½å¯èƒ½å°)
            2.45,    # å¯¹åº”é¢‘ç‡: 2.45GHz (WiFié¢‘æ®µ)
            6.5      # è¿œåŒºåœºå¢ç›Š: 6.5dBi (é«˜å¢ç›Š)
        ]

        print(f"è®¾è®¡ç›®æ ‡:")
        for i, (name, target) in enumerate(zip(self.perf_names, target_specs)):
            print(f"  {name}: {target}")

        # å‚æ•°è¾¹ç•Œï¼ˆåŸºäºæ•°æ®èŒƒå›´ï¼‰
        param_min = X_original.min(axis=0)
        param_max = X_original.max(axis=0)
        param_bounds = np.column_stack([param_min, param_max])

        print(f"\nå‚æ•°ä¼˜åŒ–è¾¹ç•Œ:")
        for i, name in enumerate(self.param_names):
            print(f"  {name}: {param_bounds[i, 0]:.3f} - {param_bounds[i, 1]:.3f}")

        # æ‰§è¡Œä¼˜åŒ–
        optimal_params, predicted_performance, optimization_loss = self.optimize_antenna(
            model, target_specs, param_bounds, num_iterations=2000
        )

        print(f"\nä¼˜åŒ–ç»“æœ:")
        print(f"æœ€ä¼˜å‚æ•°:")
        for i, name in enumerate(self.param_names):
            print(f"  {name}: {optimal_params[i]:.3f}")

        print(f"\né¢„æµ‹æ€§èƒ½:")
        for i, name in enumerate(self.perf_names):
            diff = abs(predicted_performance[i] - target_specs[i])
            status = "âœ“" if (name == self.perf_names[0] and predicted_performance[i] <= target_specs[i]) or \
                           (name == self.perf_names[1] and abs(diff) < 0.1) or \
                           (name == self.perf_names[2] and predicted_performance[i] >= target_specs[i]) else "âš ï¸"
            print(f"  {status} {name}: {predicted_performance[i]:.3f} (ç›®æ ‡: {target_specs[i]})")

        # 7. HFSSä»¿çœŸéªŒè¯
        print(f"\n7. HFSSä»¿çœŸéªŒè¯...")
        simulated_performance = self.hfss_interface(optimal_params)

        # 8. è®¾è®¡å¯è¡Œæ€§åˆ†æ
        print(f"\n8. è®¾è®¡å¯è¡Œæ€§åˆ†æ:")
        is_feasible = True

        # S11æ£€æŸ¥
        if predicted_performance[0] > -15:  # S11 > -15dB è¢«è®¤ä¸ºæ€§èƒ½è¾ƒå·®
            print(f"  âš ï¸  S11å€¼ {predicted_performance[0]:.2f}dB åé«˜ï¼Œå¯èƒ½éœ€è¦æ”¹è¿›åŒ¹é…")
            is_feasible = False
        else:
            print(f"  âœ“ S11å€¼ {predicted_performance[0]:.2f}dB æ»¡è¶³è¦æ±‚")

        # é¢‘ç‡æ£€æŸ¥
        if not (2.4 <= predicted_performance[1] <= 2.5):  # WiFi 2.4GHzé¢‘æ®µ
            print(f"  âš ï¸  å·¥ä½œé¢‘ç‡ {predicted_performance[1]:.2f}GHz ä¸åœ¨WiFi 2.4GHzé¢‘æ®µå†…")
            is_feasible = False
        else:
            print(f"  âœ“ å·¥ä½œé¢‘ç‡åœ¨WiFi 2.4GHzé¢‘æ®µå†…")

        # å¢ç›Šæ£€æŸ¥
        if predicted_performance[2] < 5.0:  # å¢ç›Šä½äº5dBi
            print(f"  âš ï¸  å¢ç›Š {predicted_performance[2]:.2f}dBi åä½")
            is_feasible = False
        else:
            print(f"  âœ“ å¢ç›Š {predicted_performance[2]:.2f}dBi æ»¡è¶³è¦æ±‚")

        end_time = time.time()
        print(f"\n=== è´´ç‰‡å¤©çº¿è®¾è®¡å·¥ä½œæµç¨‹å®Œæˆ ===")
        print(f"æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")

        if is_feasible:
            print("ğŸ‰ è®¾è®¡æˆåŠŸï¼è¯¥è´´ç‰‡å¤©çº¿è®¾è®¡æ»¡è¶³è¦æ±‚ã€‚")
        else:
            print("âš ï¸  è®¾è®¡åŸºæœ¬å®Œæˆï¼Œä½†éƒ¨åˆ†æŒ‡æ ‡éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚")

        # ä¿å­˜è®¾è®¡ç»“æœ
        design_result = {
            'optimal_parameters': optimal_params,
            'predicted_performance': predicted_performance,
            'simulated_performance': simulated_performance,
            'target_specifications': target_specs,
            'optimization_loss': optimization_loss,
            'model_type': model_type,
            'training_history': history,
            'is_feasible': is_feasible,
            'total_time': end_time - start_time
        }

        np.save('patch_antenna_results/design_result.npy', design_result)
        print("è®¾è®¡ç»“æœå·²ä¿å­˜åˆ° patch_antenna_results/design_result.npy")

        return design_result

if __name__ == "__main__":
    # æ¼”ç¤ºä½¿ç”¨
    system = PatchAntennaDesignSystem()

    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    import sys
    if len(sys.argv) > 1 and sys.argv[1].endswith('.csv'):
        csv_file = sys.argv[1]
        print(f"ä½¿ç”¨CSVæ–‡ä»¶: {csv_file}")

        # å¦‚æœæŒ‡å®šäº†åˆ—å
        param_cols = None
        perf_cols = None
        if len(sys.argv) > 3:
            param_cols = sys.argv[2].split(',')
            perf_cols = sys.argv[3].split(',')

        # æ‰§è¡Œè®¾è®¡æµç¨‹
        result = system.design_workflow(
            csv_file=csv_file,
            param_cols=param_cols,
            perf_cols=perf_cols,
            model_type='resnet',
            epochs=200
        )
    else:
        # ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œæ¼”ç¤º
        print("ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œæ¼”ç¤º (æ·»åŠ CSVæ–‡ä»¶è·¯å¾„ä½œä¸ºå‚æ•°å¯ä½¿ç”¨çœŸå®æ•°æ®)")
        result = system.design_workflow(
            model_type='resnet',
            epochs=200,
            use_synthetic_data=True
        )

    print("\nè®¾è®¡æµç¨‹å…¨éƒ¨å®Œæˆï¼")