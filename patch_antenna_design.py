"""
åŸºäºPyTorchçš„è´´ç‰‡å¤©çº¿è®¾è®¡ç³»ç»Ÿ
ä¸“é—¨é’ˆå¯¹:
- è¾“å…¥: è´´ç‰‡é•¿å®½
- è¾“å‡º: S11æœ€å°å€¼ã€å¯¹åº”é¢‘ç‡ã€è¿œåŒºåœºå¢ç›Š
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from torch.utils.data import Dataset, DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from python_hfss import *
import time
import os
from sklearn.metrics import r2_score

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class PatchAntennaDesignSystem:
    def __init__(self, device=None):
        """åˆå§‹åŒ–è´´ç‰‡å¤©çº¿è®¾è®¡ç³»ç»Ÿ"""
        self.device = device if device else torch.device(
            'cuda' if torch.cuda.is_available() else 'cpu'
        )
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")

        # ç³»ç»Ÿå‚æ•° - ä¿®æ”¹ï¼šè¾“å…¥ç»´åº¦ä»4å˜ä¸º2
        self.scaler = StandardScaler()
        self.target_scaler = MinMaxScaler()  # å¯¹ç›®æ ‡å€¼ä½¿ç”¨MinMaxScaler
        self.input_dim = 2  # åªä½¿ç”¨è´´ç‰‡é•¿åº¦å’Œå®½åº¦
        self.output_dim = 3  # S11æœ€å°å€¼ã€å¯¹åº”é¢‘ç‡ã€è¿œåŒºåœºå¢ç›Š

        # å‚æ•°å’Œæ€§èƒ½æŒ‡æ ‡åç§° - ä¿®æ”¹ï¼šåªä¿ç•™ä¸¤ä¸ªå‚æ•°
        self.param_names = ['è´´ç‰‡é•¿åº¦(mm)', 'è´´ç‰‡å®½åº¦(mm)']
        self.perf_names = ['S11æœ€å°å€¼(dB)', 'å¯¹åº”é¢‘ç‡(GHz)', 'è¿œåŒºåœºå¢ç›Š(dBi)']

    def load_csv_data(self, csv_file, param_cols=None, perf_cols=None):
        """
        ä»CSVæ–‡ä»¶åŠ è½½è´´ç‰‡å¤©çº¿æ•°æ®

        å‚æ•°:
        csv_file: CSVæ–‡ä»¶è·¯å¾„
        param_cols: å‚æ•°åˆ—ååˆ—è¡¨ (å¯é€‰)
        perf_cols: æ€§èƒ½æŒ‡æ ‡åˆ—ååˆ—è¡¨ (å¯é€‰)

        è¿”å›:
        X_scaled: å½’ä¸€åŒ–çš„å¤©çº¿å‚æ•°
        y_scaled: å½’ä¸€åŒ–çš„å¤©çº¿æ€§èƒ½æŒ‡æ ‡
        X_original: åŸå§‹å¤©çº¿å‚æ•°
        y_original: åŸå§‹æ€§èƒ½æŒ‡æ ‡
        """
        print(f"ä»CSVæ–‡ä»¶åŠ è½½æ•°æ®: {csv_file}")

        # è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv(csv_file)
        print(f"æ•°æ®å½¢çŠ¶: {df.shape}")
        print(f"åˆ—å: {list(df.columns)}")

        # å¦‚æœæ²¡æœ‰æŒ‡å®šåˆ—åï¼Œåˆ™ä½¿ç”¨é»˜è®¤åˆ—å - ä¿®æ”¹ï¼šåªä½¿ç”¨ä¸¤ä¸ªå‚æ•°
        if param_cols is None:
            param_cols = ['patch_length', 'patch_width']
            print(f"ä½¿ç”¨é»˜è®¤å‚æ•°åˆ—å: {param_cols}")

        if perf_cols is None:
            perf_cols = ['s11_min', 'freq_at_s11_min', 'far_field_gain']
            print(f"ä½¿ç”¨é»˜è®¤æ€§èƒ½åˆ—å: {perf_cols}")

        # éªŒè¯åˆ—åæ˜¯å¦å­˜åœ¨
        for col in param_cols + perf_cols:
            if col not in df.columns:
                raise ValueError(f"åˆ—å '{col}' ä¸åœ¨CSVæ–‡ä»¶ä¸­")

        # æå–æ•°æ®
        X_original = df[param_cols].values
        y_original = df[perf_cols].values

        # éªŒè¯æ•°æ®ç»´åº¦ - ä¿®æ”¹ï¼šè¾“å…¥ç»´åº¦ä¸º2
        if X_original.shape[1] != self.input_dim:
            raise ValueError(f"å‚æ•°åˆ—æ•°åº”ä¸º {self.input_dim}ï¼Œä½†å®é™…ä¸º {X_original.shape[1]}")

        if y_original.shape[1] != self.output_dim:
            raise ValueError(f"æ€§èƒ½åˆ—æ•°åº”ä¸º {self.output_dim}ï¼Œä½†å®é™…ä¸º {y_original.shape[1]}")

        # æ•°æ®å½’ä¸€åŒ–
        X_scaled = self.scaler.fit_transform(X_original)
        y_scaled = self.target_scaler.fit_transform(y_original)

        print(f"å‚æ•°æ•°æ®å½¢çŠ¶: {X_original.shape}")
        print(f"æ€§èƒ½æ•°æ®å½¢çŠ¶: {y_original.shape}")

        # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡ä¿¡æ¯
        print(f"\nå‚æ•°ç»Ÿè®¡:")
        for i, (name, col) in enumerate(zip(self.param_names, param_cols)):
            print(f"  {name}: å‡å€¼={X_original[:, i].mean():.3f}, æ ‡å‡†å·®={X_original[:, i].std():.3f}")

        print(f"\næ€§èƒ½æŒ‡æ ‡ç»Ÿè®¡:")
        for i, (name, col) in enumerate(zip(self.perf_names, perf_cols)):
            print(f"  {name}: å‡å€¼={y_original[:, i].mean():.3f}, æ ‡å‡†å·®={y_original[:, i].std():.3f}")

        return (torch.tensor(X_scaled, dtype=torch.float32),
                torch.tensor(y_scaled, dtype=torch.float32),
                X_original, y_original)

    def generate_synthetic_data(self, num_samples=10000):
        """
        ç”Ÿæˆæ›´çœŸå®çš„åˆæˆè´´ç‰‡å¤©çº¿æ•°æ®

        å‚æ•°:
        num_samples: æ ·æœ¬æ•°é‡

        è¿”å›:
        X_scaled: å½’ä¸€åŒ–çš„å¤©çº¿å‚æ•°
        y_scaled: å½’ä¸€åŒ–çš„å¤©çº¿æ€§èƒ½æŒ‡æ ‡
        X_original: åŸå§‹å¤©çº¿å‚æ•°
        y_original: åŸå§‹æ€§èƒ½æŒ‡æ ‡
        """
        np.random.seed(42)
        print(f"ç”Ÿæˆåˆæˆè´´ç‰‡å¤©çº¿æ•°æ®ï¼Œæ ·æœ¬æ•°: {num_samples}")

        # è´´ç‰‡å¤©çº¿å‚æ•°èŒƒå›´ï¼ˆåªä½¿ç”¨é•¿åº¦å’Œå®½åº¦ï¼‰
        patch_length = np.random.uniform(10, 50, num_samples)  # è´´ç‰‡é•¿åº¦ 10-50mm
        patch_width = np.random.uniform(10, 60, num_samples)   # è´´ç‰‡å®½åº¦ 10-60mm

        X_original = np.column_stack([patch_length, patch_width])

        # æ›´çœŸå®çš„æ€§èƒ½æŒ‡æ ‡è®¡ç®—ï¼ˆåŸºäºç”µç£å­¦åŸç†çš„æ”¹è¿›æ¨¡å‹ï¼‰
        c = 3e8  # å…‰é€Ÿ
        epsilon_r = 4.4  # FR4ä»‹ç”µå¸¸æ•°
        h = 0.035e-3  # ä»‹è´¨åšåº¦(m) - ä½¿ç”¨æ ‡å‡†å€¼0.035mm

        # æœ‰æ•ˆä»‹ç”µå¸¸æ•°è®¡ç®—
        epsilon_eff = (epsilon_r + 1) / 2 + (epsilon_r - 1) / 2 * (1 + 12 * h / patch_width * 1e-3) ** (-0.5)

        # è°æŒ¯é¢‘ç‡è®¡ç®—ï¼ˆæ›´å‡†ç¡®çš„å…¬å¼ï¼‰
        delta_l = 0.412 * h * (epsilon_eff + 0.3) * (patch_width * 1e-3 / h + 0.264) / \
                  ((epsilon_eff - 0.258) * (patch_width * 1e-3 / h + 0.8))
        L_eff = (patch_length * 1e-3) + 2 * delta_l
        freq = c / (2 * L_eff * np.sqrt(epsilon_eff)) / 1e9

        # æ·»åŠ åˆ¶é€ è¯¯å·®å’Œç¯å¢ƒå™ªå£°
        freq += np.random.normal(0, 0.05, num_samples)

        # S11æœ€å°å€¼è®¡ç®—ï¼ˆè€ƒè™‘é˜»æŠ—åŒ¹é…ï¼‰
        Z0 = 50  # ç³»ç»Ÿé˜»æŠ—
        # å¤©çº¿è¾“å…¥é˜»æŠ—è®¡ç®—ï¼ˆç®€åŒ–æ¨¡å‹ï¼‰
        Z_antenna = 377 * patch_width * 1e-3 / (2 * h * np.sqrt(epsilon_eff))
        # åå°„ç³»æ•°è®¡ç®—
        reflection_coeff = (Z_antenna - Z0) / (Z_antenna + Z0)
        s11_min = 20 * np.log10(np.abs(reflection_coeff))

        # æ·»åŠ å®é™…å½±å“å› ç´ 
        s11_min += np.random.normal(0, 1.5, num_samples)
        s11_min = np.clip(s11_min, -40, -5)  # S11èŒƒå›´é™åˆ¶åœ¨-40åˆ°-5dB

        # è¿œåŒºåœºå¢ç›Šè®¡ç®—ï¼ˆæ›´å‡†ç¡®çš„æ¨¡å‹ï¼‰
        gain = 2.15 + 0.01 * (patch_length + patch_width) + 0.5 * np.log10(patch_length * patch_width) + np.random.normal(0, 0.4, num_samples)
        gain = np.clip(gain, 1, 12)  # å¢ç›ŠèŒƒå›´é™åˆ¶åœ¨1-12dBi

        y_original = np.column_stack([s11_min, freq, gain])

        # æ•°æ®å½’ä¸€åŒ–
        X_scaled = self.scaler.fit_transform(X_original)
        y_scaled = self.target_scaler.fit_transform(y_original)

        print(f"åˆæˆæ•°æ®ç”Ÿæˆå®Œæˆ")
        print(f"å‚æ•°æ•°æ®å½¢çŠ¶: {X_original.shape}")
        print(f"æ€§èƒ½æ•°æ®å½¢çŠ¶: {y_original.shape}")

        return (torch.tensor(X_scaled, dtype=torch.float32),
                torch.tensor(y_scaled, dtype=torch.float32),
                X_original, y_original)

    def create_model(self, model_type='resnet'):
        """
        åˆ›å»ºæ”¹è¿›çš„è´´ç‰‡å¤©çº¿è®¾è®¡ç¥ç»ç½‘ç»œæ¨¡å‹

        å‚æ•°:
        model_type: æ¨¡å‹ç±»å‹ ('mlp', 'resnet', 'cnn', 'rnn', 'gnn')

        è¿”å›:
        ç¥ç»ç½‘ç»œæ¨¡å‹
        """
        if model_type == 'mlp':
            return nn.Sequential(
                nn.Linear(self.input_dim, 128),  # ä¿®æ”¹ï¼šè¾“å…¥ç»´åº¦ä¸º2
                nn.BatchNorm1d(128),
                nn.ReLU(),
                nn.Dropout(0.3),

                nn.Linear(128, 256),
                nn.BatchNorm1d(256),
                nn.ReLU(),
                nn.Dropout(0.4),

                nn.Linear(256, 128),
                nn.BatchNorm1d(128),
                nn.ReLU(),
                nn.Dropout(0.3),

                nn.Linear(128, 64),
                nn.BatchNorm1d(64),
                nn.ReLU(),
                nn.Dropout(0.2),

                nn.Linear(64, self.output_dim)
            ).to(self.device)

        elif model_type == 'resnet':
            # ä¿®å¤çš„æ®‹å·®ç½‘ç»œ
            class ResidualBlock(nn.Module):
                def __init__(self, in_dim, out_dim, stride=1):
                    super().__init__()
                    self.conv1 = nn.Linear(in_dim, out_dim)
                    self.bn1 = nn.BatchNorm1d(out_dim)
                    self.relu = nn.ReLU()
                    self.conv2 = nn.Linear(out_dim, out_dim)
                    self.bn2 = nn.BatchNorm1d(out_dim)

                    self.downsample = None
                    if stride != 1 or in_dim != out_dim:
                        self.downsample = nn.Sequential(
                            nn.Linear(in_dim, out_dim),
                            nn.BatchNorm1d(out_dim)
                        )

                def forward(self, x):
                    residual = x
                    if self.downsample is not None:
                        residual = self.downsample(x)

                    out = self.conv1(x)
                    out = self.bn1(out)
                    out = self.relu(out)
                    out = self.conv2(out)
                    out = self.bn2(out)

                    out += residual
                    out = self.relu(out)
                    return out

            return nn.Sequential(
                nn.Linear(self.input_dim, 64),  # ä¿®æ”¹ï¼šè¾“å…¥ç»´åº¦ä¸º2
                nn.BatchNorm1d(64),
                nn.ReLU(),

                ResidualBlock(64, 64),
                ResidualBlock(64, 128, stride=2),
                ResidualBlock(128, 128),
                ResidualBlock(128, 256, stride=2),
                ResidualBlock(256, 256),

                nn.Linear(256, 128),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(128, self.output_dim)
            ).to(self.device)

        elif model_type == 'cnn':
            # æ”¹è¿›çš„ä¸€ç»´å·ç§¯ç½‘ç»œ
            class ImprovedCNN(nn.Module):
                def __init__(self, input_dim, output_dim):
                    super().__init__()

                    # ç‰¹å¾æå–éƒ¨åˆ†
                    self.features = nn.Sequential(
                        # ç¬¬ä¸€å±‚ï¼šæ‰©å±•ç»´åº¦
                        nn.Linear(input_dim, 32),  # ä¿®æ”¹ï¼šè¾“å…¥ç»´åº¦ä¸º2
                        nn.BatchNorm1d(32),
                        nn.ReLU(),
                        nn.Dropout(0.2),

                        # è½¬æ¢ä¸ºå·ç§¯æ ¼å¼
                        nn.Unflatten(1, (1, 32)),

                        # å·ç§¯å—1
                        nn.Conv1d(1, 16, kernel_size=3, padding=1),
                        nn.BatchNorm1d(16),
                        nn.ReLU(),
                        nn.MaxPool1d(2),

                        # å·ç§¯å—2
                        nn.Conv1d(16, 32, kernel_size=3, padding=1),
                        nn.BatchNorm1d(32),
                        nn.ReLU(),
                        nn.MaxPool1d(2),

                        # å·ç§¯å—3
                        nn.Conv1d(32, 64, kernel_size=3, padding=1),
                        nn.BatchNorm1d(64),
                        nn.ReLU(),
                        nn.AdaptiveMaxPool1d(4),

                        # å±•å¹³
                        nn.Flatten()
                    )

                    # åˆ†ç±»/å›å½’å¤´éƒ¨
                    self.head = nn.Sequential(
                        nn.Linear(64 * 4, 256),
                        nn.BatchNorm1d(256),
                        nn.ReLU(),
                        nn.Dropout(0.4),

                        nn.Linear(256, 128),
                        nn.BatchNorm1d(128),
                        nn.ReLU(),
                        nn.Dropout(0.3),

                        nn.Linear(128, 64),
                        nn.BatchNorm1d(64),
                        nn.ReLU(),
                        nn.Dropout(0.2),

                        nn.Linear(64, output_dim)
                    )

                def forward(self, x):
                    x = self.features(x)
                    x = self.head(x)
                    return x

            return ImprovedCNN(
                input_dim=self.input_dim,
                output_dim=self.output_dim
            ).to(self.device)

        elif model_type == 'rnn':
            # æ”¹è¿›çš„å¾ªç¯ç¥ç»ç½‘ç»œ
            class ImprovedRNN(nn.Module):
                def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2):
                    super().__init__()
                    self.hidden_dim = hidden_dim
                    self.num_layers = num_layers

                    # LSTMå±‚
                    self.lstm = nn.LSTM(
                        input_size=input_dim,
                        hidden_size=hidden_dim,
                        num_layers=num_layers,
                        batch_first=True,
                        bidirectional=True,
                        dropout=0.3
                    )

                    # æ‰¹å½’ä¸€åŒ–å±‚
                    self.batch_norm = nn.BatchNorm1d(hidden_dim * 2)

                    # æ³¨æ„åŠ›æœºåˆ¶
                    self.attention = nn.Linear(hidden_dim * 2, 1)

                    # å…¨è¿æ¥å±‚
                    self.fc1 = nn.Linear(hidden_dim * 2, 128)
                    self.fc2 = nn.Linear(128, 64)
                    self.fc3 = nn.Linear(64, output_dim)

                    # æ¿€æ´»å‡½æ•°å’Œ dropout
                    self.relu = nn.ReLU()
                    self.dropout = nn.Dropout(0.4)
                    self.leaky_relu = nn.LeakyReLU(0.2)

                def forward(self, x):
                    # x: (batch_size, input_dim) -> (batch_size, seq_len, input_dim)
                    x = x.unsqueeze(1)  # å¯¹äºé™æ€å‚æ•°ï¼Œseq_len=1

                    # LSTMå‰å‘ä¼ æ’­
                    batch_size = x.size(0)
                    h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_dim,
                                   device=x.device)
                    c0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_dim,
                                   device=x.device)

                    out, _ = self.lstm(x, (h0, c0))  # out: (batch_size, seq_len, hidden_dim * 2)

                    # æ³¨æ„åŠ›æœºåˆ¶
                    attention_scores = self.attention(out).squeeze(2)
                    attention_weights = torch.softmax(attention_scores, dim=1)
                    attended_out = torch.bmm(attention_weights.unsqueeze(1), out).squeeze(1)

                    # æ‰¹å½’ä¸€åŒ–å’Œå…¨è¿æ¥å±‚
                    out = self.batch_norm(attended_out)
                    out = self.dropout(self.relu(self.fc1(out)))
                    out = self.dropout(self.relu(self.fc2(out)))
                    out = self.fc3(out)

                    return out

            return ImprovedRNN(
                input_dim=self.input_dim,
                hidden_dim=64,
                output_dim=self.output_dim,
                num_layers=2
            ).to(self.device)

        elif model_type == 'gnn':
            # ä¿®å¤çš„å›¾ç¥ç»ç½‘ç»œ
            class AdvancedGNN(nn.Module):
                def __init__(self, input_dim, output_dim, hidden_dim=32, num_layers=3, num_heads=2):
                    super().__init__()
                    self.input_dim = input_dim
                    self.hidden_dim = hidden_dim
                    self.num_layers = num_layers
                    self.num_heads = num_heads
                    self.total_hidden_dim = hidden_dim * num_heads  # å¤šå¤´æ³¨æ„åŠ›æ€»ç»´åº¦

                    # å®šä¹‰å¤©çº¿å‚æ•°å›¾ç»“æ„ï¼ˆ2ä¸ªèŠ‚ç‚¹ï¼šé•¿åº¦å’Œå®½åº¦ï¼‰
                    self.edge_index = torch.tensor([
                        [0, 1],  # æºèŠ‚ç‚¹
                        [1, 0]   # ç›®æ ‡èŠ‚ç‚¹
                    ], dtype=torch.long)

                    # èŠ‚ç‚¹ç‰¹å¾åµŒå…¥
                    self.node_embedding = nn.Linear(1, hidden_dim)

                    # å¤šå¤´æ³¨æ„åŠ›GATå±‚
                    self.gat_layers = nn.ModuleList()
                    for i in range(num_layers):
                        in_dim = hidden_dim if i == 0 else self.total_hidden_dim
                        self.gat_layers.append(nn.ModuleList([
                            nn.Linear(in_dim, hidden_dim) for _ in range(num_heads)
                        ]))

                    # æ³¨æ„åŠ›åˆ†æ•°è®¡ç®—
                    self.attention_layers = nn.ModuleList([
                        nn.Linear(2 * hidden_dim, 1) for _ in range(num_layers)
                    ])

                    # å›¾å·ç§¯å±‚
                    self.graph_conv_layers = nn.ModuleList([
                        nn.Linear(self.total_hidden_dim, self.total_hidden_dim) for _ in range(num_layers)
                    ])

                    # æ‰¹å½’ä¸€åŒ–å’Œæ®‹å·®è¿æ¥
                    self.batch_norm_layers = nn.ModuleList([
                        nn.BatchNorm1d(self.total_hidden_dim) for _ in range(num_layers)
                    ])

                    self.residual_layers = nn.ModuleList([
                        nn.Linear(self.total_hidden_dim, self.total_hidden_dim) if i > 0 else None
                        for i in range(num_layers)
                    ])

                    # å…¨å±€æ± åŒ–å’Œè¾“å‡ºå±‚
                    self.global_pool = nn.AdaptiveAvgPool1d(1)
                    self.fc1 = nn.Linear(self.total_hidden_dim, 256)
                    self.fc2 = nn.Linear(256, 128)
                    self.fc3 = nn.Linear(128, output_dim)

                    # æ¿€æ´»å‡½æ•°å’Œæ­£åˆ™åŒ–
                    self.relu = nn.ReLU()
                    self.dropout = nn.Dropout(0.4)
                    self.leaky_relu = nn.LeakyReLU(0.2)

                def compute_multi_head_attention(self, node_features, edge_index, layer_idx):
                    """è®¡ç®—å¤šå¤´æ³¨æ„åŠ›æƒé‡"""
                    src, dst = edge_index

                    all_head_outputs = []

                    for head in range(self.num_heads):
                        # è·å–å½“å‰å¤´çš„çº¿æ€§å˜æ¢
                        linear = self.gat_layers[layer_idx][head]
                        transformed_features = linear(node_features)

                        # è·å–æºèŠ‚ç‚¹å’Œç›®æ ‡èŠ‚ç‚¹çš„ç‰¹å¾
                        src_features = transformed_features[src]
                        dst_features = transformed_features[dst]

                        # è¿æ¥ç‰¹å¾è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
                        cat_features = torch.cat([src_features, dst_features], dim=1)
                        attention_scores = self.attention_layers[layer_idx](cat_features)
                        attention_scores = self.leaky_relu(attention_scores)

                        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
                        num_nodes = node_features.size(0)
                        attention_weights = torch.zeros(num_nodes, num_nodes, device=node_features.device)
                        attention_weights[src, dst] = attention_scores.squeeze()

                        # è¡Œå½’ä¸€åŒ–
                        row_sums = attention_weights.sum(dim=1, keepdim=True)
                        attention_weights = attention_weights / (row_sums + 1e-12)

                        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
                        attended_features = torch.matmul(attention_weights, transformed_features)
                        all_head_outputs.append(attended_features)

                    # æ‹¼æ¥å¤šå¤´ç»“æœ
                    multi_head_output = torch.cat(all_head_outputs, dim=1)
                    return multi_head_output

                def forward(self, x):
                    """
                    GNNå‰å‘ä¼ æ’­
                    x: (batch_size, input_dim) - å¤©çº¿å‚æ•°
                    """
                    batch_size = x.size(0)

                    # å°†è¾“å…¥è½¬æ¢ä¸ºèŠ‚ç‚¹ç‰¹å¾çŸ©é˜µ
                    node_features = x.view(batch_size * self.input_dim, 1)
                    node_features = self.node_embedding(node_features)
                    node_features = self.relu(node_features)
                    node_features = self.dropout(node_features)

                    # å¤åˆ¶è¾¹ç´¢å¼•ä»¥é€‚åº”æ‰¹å¤„ç†
                    edge_index = self.edge_index.clone()
                    for i in range(1, batch_size):
                        edge_index = torch.cat([
                            edge_index,
                            self.edge_index + i * self.input_dim
                        ], dim=1)

                    # GNNå±‚å †å 
                    for layer_idx in range(self.num_layers):
                        # å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
                        attention_output = self.compute_multi_head_attention(
                            node_features, edge_index, layer_idx
                        )

                        # å›¾å·ç§¯æ“ä½œ
                        conv_output = self.graph_conv_layers[layer_idx](attention_output)

                        # æ®‹å·®è¿æ¥
                        if self.residual_layers[layer_idx] is not None:
                            conv_output += self.residual_layers[layer_idx](attention_output)

                        # æ‰¹å½’ä¸€åŒ–å’Œæ¿€æ´»
                        conv_output = self.batch_norm_layers[layer_idx](conv_output)
                        conv_output = self.relu(conv_output)
                        conv_output = self.dropout(conv_output)

                        node_features = conv_output

                    # å…¨å±€æ± åŒ–
                    node_features = node_features.view(batch_size, self.input_dim, -1)
                    pooled_features = self.global_pool(node_features.permute(0, 2, 1)).squeeze()

                    # è¾“å‡ºå±‚
                    out = self.fc1(pooled_features)
                    out = self.relu(out)
                    out = self.dropout(out)

                    out = self.fc2(out)
                    out = self.relu(out)
                    out = self.dropout(out)

                    out = self.fc3(out)

                    return out

            return AdvancedGNN(
                input_dim=self.input_dim,
                output_dim=self.output_dim,
                hidden_dim=32,
                num_layers=3,
                num_heads=2
            ).to(self.device)

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")

    def train_model(self, model, X_train, y_train, X_val, y_val,
                   epochs=300, batch_size=128, lr=0.001):
        """
        æ”¹è¿›çš„æ¨¡å‹è®­ç»ƒæ–¹æ³•

        å‚æ•°:
        model: ç¥ç»ç½‘ç»œæ¨¡å‹
        X_train, y_train: è®­ç»ƒæ•°æ®
        X_val, y_val: éªŒè¯æ•°æ®
        epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹æ¬¡å¤§å°
        lr: å­¦ä¹ ç‡

        è¿”å›:
        è®­ç»ƒå†å²
        """
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataset = TensorDataset(X_train, y_train)
        val_dataset = TensorDataset(X_val, y_val)

        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

        # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = nn.MSELoss()
        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            optimizer, T_0=50, T_mult=2, eta_min=1e-6
        )

        # è®­ç»ƒå†å²
        history = {
            'train_loss': [],
            'val_loss': [],
            'train_rmse': [],
            'val_rmse': [],
            'learning_rate': []
        }

        best_val_loss = float('inf')
        patience = 30  # æ—©åœç­–ç•¥
        early_stop_counter = 0

        print("å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        for epoch in range(epochs):
            model.train()
            train_loss = 0.0

            for inputs, targets in train_loader:
                inputs, targets = inputs.to(self.device), targets.to(self.device)

                # å‰å‘ä¼ æ’­
                outputs = model(inputs)
                loss = criterion(outputs, targets)

                # åå‘ä¼ æ’­å’Œä¼˜åŒ–
                optimizer.zero_grad()
                loss.backward()

                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

                optimizer.step()
                scheduler.step(epoch + len(train_loader.dataset) / len(train_loader))

                train_loss += loss.item() * inputs.size(0)

            # è®¡ç®—å¹³å‡æŸå¤±
            train_loss /= len(train_loader.dataset)
            train_rmse = np.sqrt(train_loss)

            # éªŒè¯
            model.eval()
            val_loss = 0.0

            with torch.no_grad():
                for inputs, targets in val_loader:
                    inputs, targets = inputs.to(self.device), targets.to(self.device)
                    outputs = model(inputs)
                    loss = criterion(outputs, targets)
                    val_loss += loss.item() * inputs.size(0)

            val_loss /= len(val_loader.dataset)
            val_rmse = np.sqrt(val_loss)

            # è®°å½•å­¦ä¹ ç‡
            current_lr = optimizer.param_groups[0]['lr']

            # è®°å½•å†å²
            history['train_loss'].append(train_loss)
            history['val_loss'].append(val_loss)
            history['train_rmse'].append(train_rmse)
            history['val_rmse'].append(val_rmse)
            history['learning_rate'].append(current_lr)

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save(model.state_dict(), 'best_patch_antenna_model.pth')
                early_stop_counter = 0
            else:
                early_stop_counter += 1
                if early_stop_counter >= patience:
                    print(f"æ—©åœç­–ç•¥è§¦å‘ï¼Œåœ¨ç¬¬ {epoch+1} è½®åœæ­¢è®­ç»ƒ")
                    break

            # æ‰“å°è¿›åº¦
            if (epoch + 1) % 10 == 0:
                print(f"Epoch [{epoch+1}/{epochs}], "
                      f"Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}, "
                      f"Train RMSE: {train_rmse:.6f}, Val RMSE: {val_rmse:.6f}, "
                      f"LR: {current_lr:.6f}")

        print(f"è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
        return history

    def optimize_antenna(self, model, target_specs, param_bounds,
                        num_iterations=3000, learning_rate=0.01, device=None):
        """
        æ”¹è¿›çš„è´´ç‰‡å¤©çº¿å‚æ•°ä¼˜åŒ–

        å‚æ•°:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        target_specs: ç›®æ ‡æ€§èƒ½æŒ‡æ ‡ [S11æœ€å°å€¼, å¯¹åº”é¢‘ç‡, è¿œåŒºåœºå¢ç›Š]
        param_bounds: å‚æ•°è¾¹ç•Œ [[min1, max1], [min2, max2]] - ä¿®æ”¹ï¼šåªæœ‰ä¸¤ä¸ªå‚æ•°
        num_iterations: è¿­ä»£æ¬¡æ•°
        learning_rate: å­¦ä¹ ç‡
        device: è®¡ç®—è®¾å¤‡ï¼ˆGPU/CPUï¼‰
        """
        # 1. å¼ºåˆ¶æŒ‡å®šè®¾å¤‡
        if device is None:
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ä¼˜åŒ–ä½¿ç”¨è®¾å¤‡: {device}")

        # 2. éªŒè¯è¾“å…¥ - ä¿®æ”¹ï¼šæ£€æŸ¥å‚æ•°è¾¹ç•Œç»´åº¦
        if len(target_specs) != self.output_dim:
            raise ValueError(f"ç›®æ ‡æ€§èƒ½æŒ‡æ ‡åº”ä¸º {self.output_dim} ä¸ªï¼Œå®é™…ä¸º {len(target_specs)}")
        if param_bounds.shape != (self.input_dim, 2):
            raise ValueError(f"å‚æ•°è¾¹ç•Œåº”ä¸º {self.input_dim}x2 çš„æ•°ç»„ï¼Œå®é™…ä¸º {param_bounds.shape}")

        num_params = self.input_dim
        model = model.to(device)

        # 3. åˆ›å»ºä¼˜åŒ–å‚æ•°
        params = torch.rand(num_params, dtype=torch.float32, device=device, requires_grad=True)

        # 4. å‚æ•°æ˜ å°„åˆ°æŒ‡å®šèŒƒå›´
        param_min = torch.tensor(param_bounds[:, 0], dtype=torch.float32, device=device)
        param_max = torch.tensor(param_bounds[:, 1], dtype=torch.float32, device=device)
        params.data = params.data * (param_max - param_min) + param_min

        # 5. ä¼˜åŒ–å™¨
        optimizer = optim.Adam([params], lr=learning_rate, betas=(0.9, 0.999), eps=1e-8)

        # ç›®æ ‡è§„æ ¼å½’ä¸€åŒ–
        target_tensor = torch.tensor(
            self.target_scaler.transform([target_specs]),
            dtype=torch.float32,
            device=device
        ).squeeze()

        best_loss = float('inf')
        best_params = None
        best_performance = None

        print("å¼€å§‹è´´ç‰‡å¤©çº¿å‚æ•°ä¼˜åŒ–...")
        print(f"ç›®æ ‡æ€§èƒ½: S11={target_specs[0]:.2f}dB, é¢‘ç‡={target_specs[1]:.2f}GHz, å¢ç›Š={target_specs[2]:.2f}dBi")

        # 6. æ¨¡å‹æ¨¡å¼è®¾ç½®
        model.train()
        for p in model.parameters():
            p.requires_grad = False
            if isinstance(p, (nn.GRU, nn.LSTM, nn.RNN)):
                p.flatten_parameters()

        # ç‰¹æ®Šå¤„ç†batch norm
        def set_batch_norm_eval(model):
            for module in model.modules():
                if isinstance(module, nn.BatchNorm1d):
                    module.eval()

        set_batch_norm_eval(model)

        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_iterations)

        for i in range(num_iterations):
            optimizer.zero_grad()

            # å½’ä¸€åŒ–å‚æ•°
            params_normalized = (params - param_min) / (param_max - param_min + 1e-8)
            params_normalized = torch.clamp(params_normalized, 0.0, 1.0)

            # æ¨¡å‹é¢„æµ‹
            performance = model(params_normalized.unsqueeze(0))[0]

            # ç¡®ä¿æ€§èƒ½è¾“å‡ºæ˜¯æ­£ç¡®çš„å½¢çŠ¶ (3,)
            if performance.dim() != 1 or performance.shape[0] != self.output_dim:
                performance = performance.view(-1)[:self.output_dim]  # è°ƒæ•´å½¢çŠ¶
                if performance.shape[0] < self.output_dim:
                    # å¦‚æœè¾“å‡ºç»´åº¦ä¸è¶³ï¼Œå¡«å……é»˜è®¤å€¼
                    pad_size = self.output_dim - performance.shape[0]
                    performance = torch.cat([performance, torch.zeros(pad_size, device=device)])

            # è®¡ç®—åŠ æƒæŸå¤±ï¼ˆè€ƒè™‘ä¸åŒæŒ‡æ ‡çš„é‡è¦æ€§ï¼‰
            weights = torch.tensor([3.0, 1.5, 2.0], dtype=torch.float32, device=device)
            loss = torch.mean(weights * torch.square(performance - target_tensor))

            # åå‘ä¼ æ’­
            try:
                loss.backward()
            except RuntimeError as e:
                print(f"åå‘ä¼ æ’­é”™è¯¯: {e}")
                torch.backends.cudnn.enabled = False
                loss.backward()
                torch.backends.cudnn.enabled = True

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_([params], max_norm=0.5)

            # æ›´æ–°å‚æ•°
            optimizer.step()
            scheduler.step()

            # é™åˆ¶å‚æ•°è¾¹ç•Œ
            with torch.no_grad():
                params.clamp_(param_min, param_max)

            # æ›´æ–°æœ€ä½³ç»“æœ
            current_loss = loss.item()
            if current_loss < best_loss:
                best_loss = current_loss
                best_params = params.detach().cpu().numpy().copy()
                best_performance = performance.detach().cpu().numpy().copy()

            # æ‰“å°è¿›åº¦
            if (i + 1) % 200 == 0 or i == num_iterations - 1:
                # åå½’ä¸€åŒ–æ˜¾ç¤ºçœŸå®å€¼
                if best_performance is not None and len(best_performance) == self.output_dim:
                    try:
                        pred_real = self.target_scaler.inverse_transform(
                            best_performance.reshape(1, -1)
                        )[0]
                        print(f"Iteration {i + 1}/{num_iterations}, Loss: {current_loss:.6f}, "
                              f"Best Loss: {best_loss:.6f}, "
                              f"Best S11: {pred_real[0]:.2f}dB, "
                              f"Best Gain: {pred_real[1]:.2f}dB")
                    except Exception as e:
                        print(f"åå½’ä¸€åŒ–é”™è¯¯: {e}")

        # æ¢å¤æ¨¡å‹çŠ¶æ€
        model.train()
        for p in model.parameters():
            p.requires_grad = True

        # åå½’ä¸€åŒ–æœ€ä½³æ€§èƒ½
        if best_performance is not None and len(best_performance) == self.output_dim:
            try:
                best_performance_real = self.target_scaler.inverse_transform(
                    best_performance.reshape(1, -1)
                )[0]
            except Exception as e:
                print(f"æœ€ç»ˆåå½’ä¸€åŒ–é”™è¯¯: {e}")
                best_performance_real = np.zeros(self.output_dim)
        else:
            best_performance_real = np.zeros(self.output_dim)

        print(f"ä¼˜åŒ–ç»“æŸï¼æœ€ä½³æŸå¤±: {best_loss:.6f}")
        return best_params, best_performance_real, best_loss

    def visualize_results(self, history, y_true, y_pred, model_type):
        """
        æ”¹è¿›çš„å¯è§†åŒ–ç»“æœ
        """
        os.makedirs('patch_antenna_results', exist_ok=True)

        # åå½’ä¸€åŒ–æ•°æ®
        y_true_real = self.target_scaler.inverse_transform(y_true)
        y_pred_real = self.target_scaler.inverse_transform(y_pred)

        # 1. ç»¼åˆè®­ç»ƒç›‘æ§å›¾
        plt.figure(figsize=(15, 10))

        # æŸå¤±æ›²çº¿
        plt.subplot(2, 2, 1)
        plt.plot(history['train_loss'], label='è®­ç»ƒæŸå¤±')
        plt.plot(history['val_loss'], label='éªŒè¯æŸå¤±')
        plt.xlabel('Epoch')
        plt.ylabel('MSEæŸå¤±')
        plt.title('è®­ç»ƒæŸå¤±æ›²çº¿')
        plt.legend()
        plt.grid(True)

        # RMSEæ›²çº¿
        plt.subplot(2, 2, 2)
        plt.plot(history['train_rmse'], label='è®­ç»ƒRMSE')
        plt.plot(history['val_rmse'], label='éªŒè¯RMSE')
        plt.xlabel('Epoch')
        plt.ylabel('RMSE')
        plt.title('è®­ç»ƒRMSEæ›²çº¿')
        plt.legend()
        plt.grid(True)

        # å­¦ä¹ ç‡å˜åŒ–
        plt.subplot(2, 2, 3)
        plt.plot(history['learning_rate'])
        plt.xlabel('Epoch')
        plt.ylabel('å­¦ä¹ ç‡')
        plt.title('å­¦ä¹ ç‡è°ƒåº¦')
        plt.grid(True)

        # RÂ²åˆ†æ•°æ¼”å˜
        plt.subplot(2, 2, 4)
        r2_scores = []
        for i in range(len(history['val_loss'])):
            r2 = max(0, 1 - history['val_loss'][i] / np.var(y_true))
            r2_scores.append(r2)
        plt.plot(r2_scores)
        plt.xlabel('Epoch')
        plt.ylabel('RÂ²åˆ†æ•°')
        plt.title('æ¨¡å‹æ€§èƒ½æ¼”å˜')
        plt.grid(True)

        plt.tight_layout()
        plt.savefig(f'patch_antenna_results/{model_type}_training_monitor.png', dpi=300, bbox_inches='tight')
        plt.close()

        # 2. é¢„æµ‹æ€§èƒ½è¯¦ç»†åˆ†æ
        fig, axes = plt.subplots(2, self.output_dim, figsize=(15, 10))

        for i in range(self.output_dim):
            # æ•£ç‚¹å›¾
            ax1 = axes[0, i]
            ax1.scatter(y_true_real[:, i], y_pred_real[:, i], alpha=0.6, s=15)
            min_val = min(y_true_real[:, i].min(), y_pred_real[:, i].min())
            max_val = max(y_true_real[:, i].max(), y_pred_real[:, i].max())
            ax1.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)
            ax1.set_xlabel('çœŸå®å€¼')
            ax1.set_ylabel('é¢„æµ‹å€¼')
            ax1.set_title(f'{self.perf_names[i]} é¢„æµ‹ vs çœŸå®')
            ax1.grid(True)

            # RÂ²åˆ†æ•°
            r2 = r2_score(y_true_real[:, i], y_pred_real[:, i])
            ax1.text(0.05, 0.95, f'RÂ² = {r2:.3f}', transform=ax1.transAxes,
                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

            # è¯¯å·®åˆ†å¸ƒ
            ax2 = axes[1, i]
            errors = y_true_real[:, i] - y_pred_real[:, i]
            ax2.hist(errors, bins=50, alpha=0.7, edgecolor='black')
            ax2.set_xlabel('è¯¯å·®')
            ax2.set_ylabel('é¢‘æ¬¡')
            ax2.set_title(f'{self.perf_names[i]} è¯¯å·®åˆ†å¸ƒ')
            ax2.grid(True)

            # ç»Ÿè®¡ä¿¡æ¯
            mean_err = np.mean(errors)
            std_err = np.std(errors)
            ax2.axvline(mean_err, color='red', linestyle='--', label=f'å‡å€¼: {mean_err:.3f}')
            ax2.axvline(mean_err + 2*std_err, color='orange', linestyle='--', label=f'Â±2Ïƒ: {std_err:.3f}')
            ax2.axvline(mean_err - 2*std_err, color='orange', linestyle='--')
            ax2.legend()

        plt.tight_layout()
        plt.savefig(f'patch_antenna_results/{model_type}_performance_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()

        # 3. å‚æ•°é‡è¦æ€§åˆ†æï¼ˆé’ˆå¯¹GNNï¼‰
        if model_type == 'gnn':
            fig, ax = plt.subplots(1, 1, figsize=(10, 6))
            param_importance = np.random.rand(self.input_dim)  # å®é™…åº”è¯¥ä»æ¨¡å‹ä¸­æå–

            bars = ax.bar(range(self.input_dim), param_importance, alpha=0.7)
            ax.set_xticks(range(self.input_dim))
            ax.set_xticklabels(self.param_names, rotation=45, ha='right')
            ax.set_ylabel('é‡è¦æ€§åˆ†æ•°')
            ax.set_title('è´´ç‰‡å¤©çº¿å‚æ•°é‡è¦æ€§åˆ†æ')
            ax.grid(True, alpha=0.3)

            # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, bar in enumerate(bars):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                       f'{height:.3f}', ha='center', va='bottom')

            plt.tight_layout()
            plt.savefig(f'patch_antenna_results/{model_type}_param_importance.png', dpi=300, bbox_inches='tight')
            plt.close()

        print(f"å¯è§†åŒ–ç»“æœå·²ä¿å­˜åˆ° patch_antenna_results ç›®å½• (æ¨¡å‹ç±»å‹: {model_type})")

    def hfss_interface(self, parameters):
        """
        HFSSä»¿çœŸæ¥å£
        """
        print("\n=== HFSSä»¿çœŸæ¥å£ ===")
        print("å¤©çº¿ç±»å‹: è´´ç‰‡å¤©çº¿")
        print(f"è®¾è®¡å‚æ•°: {parameters}")

        print("å‚æ•°è¯´æ˜:")
        for i, name in enumerate(self.param_names):
            print(f"  {name}: {parameters[i]:.3f}")

        print("\næ­£åœ¨è°ƒç”¨HFSSæ‰§è¡Œä»¥ä¸‹æ“ä½œ:")
        print("1. åˆ›å»ºæ–°çš„HFSSé¡¹ç›®")
        print("2. æ ¹æ®å‚æ•°å»ºç«‹è´´ç‰‡å¤©çº¿æ¨¡å‹")
        print("3. è®¾ç½®æ ‡å‡†GNDç»“æ„(0.035mm)")
        print("4. è®¾ç½®ä»¿çœŸé¢‘ç‡èŒƒå›´å’Œè¾¹ç•Œæ¡ä»¶")
        print("5. è¿è¡Œç”µç£ä»¿çœŸ")
        print("6. æå–S11å‚æ•°å’Œè¿œåŒºåœºå¢ç›Š")

        # æ›´çœŸå®çš„ä»¿çœŸç»“æœæ¨¡æ‹Ÿ
        patch_length, patch_width = parameters  # ä¿®æ”¹ï¼šåªæœ‰ä¸¤ä¸ªå‚æ•°

        # åŸºäºæ”¹è¿›çš„ç”µç£å­¦æ¨¡å‹
        epsilon_r = 4.4
        h = 0.035e-3  # ä½¿ç”¨æ ‡å‡†GNDåšåº¦0.035mm
        L_meters = patch_length * 1e-3

        # æ›´å‡†ç¡®çš„è°æŒ¯é¢‘ç‡è®¡ç®—
        epsilon_eff = (epsilon_r + 1) / 2 + (epsilon_r - 1) / 2 * (1 + 12 * h / (patch_width * 1e-3)) ** (-0.5)
        delta_l = 0.412 * h * (epsilon_eff + 0.3) * ((patch_width * 1e-3) / h + 0.264) / \
                  ((epsilon_eff - 0.258) * ((patch_width * 1e-3) / h + 0.8))
        L_eff = L_meters + 2 * delta_l
        freq = 3e8 / (2 * L_eff * np.sqrt(epsilon_eff)) / 1e9

        # S11è®¡ç®—
        Z0 = 50
        Z_antenna = 377 * (patch_width * 1e-3) / (2 * h * np.sqrt(epsilon_eff))
        reflection_coeff = (Z_antenna - Z0) / (Z_antenna + Z0)
        s11_min = 20 * np.log10(np.abs(reflection_coeff))

        # å¢ç›Šè®¡ç®—
        gain = 2.15 + 0.01 * (patch_length + patch_width) + 0.5 * np.log10(patch_length * patch_width)

        # æ·»åŠ ä¸€äº›å™ªå£°æ¨¡æ‹Ÿå®é™…ä»¿çœŸè¯¯å·®
        simulated_s11 = s11_min + np.random.normal(0, 0.8)
        simulated_freq = freq + np.random.normal(0, 0.02)
        simulated_gain = gain + np.random.normal(0, 0.3)

        simulated_performance = [simulated_s11, simulated_freq, simulated_gain]

        print(f"\nHFSSä»¿çœŸç»“æœ:")
        print(f"  S11æœ€å°å€¼: {simulated_performance[0]:.2f} dB")
        print(f"  å¯¹åº”é¢‘ç‡: {simulated_performance[1]:.2f} GHz")
        print(f"  è¿œåŒºåœºå¢ç›Š: {simulated_performance[2]:.2f} dBi")

        return simulated_performance

    def design_workflow(self, csv_file=None, param_cols=None, perf_cols=None,
                       model_type='resnet', epochs=300, use_synthetic_data=False):
        """
        å®Œæ•´çš„è´´ç‰‡å¤©çº¿è®¾è®¡å·¥ä½œæµç¨‹
        """
        print("=== è´´ç‰‡å¤©çº¿è®¾è®¡å·¥ä½œæµç¨‹ ===")
        print("=" * 60)
        start_time = time.time()

        # 1. åŠ è½½æ•°æ®
        print("\n1. åŠ è½½å¤©çº¿æ•°æ®...")
        if csv_file and not use_synthetic_data:
            X_scaled, y_scaled, X_original, y_original = self.load_csv_data(
                csv_file, param_cols, perf_cols
            )
        else:
            print("ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œæ¼”ç¤º")
            X_scaled, y_scaled, X_original, y_original = self.generate_synthetic_data(
                num_samples=10000
            )

        print(f"æ•°æ®é›†å¤§å°: {X_scaled.shape[0]} æ ·æœ¬")

        # 2. åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        X_train, X_val, y_train, y_val = train_test_split(
            X_scaled, y_scaled, test_size=0.2, random_state=42
        )

        # 3. åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
        print(f"\n2. åˆ›å»º {model_type} æ¨¡å‹...")
        model = self.create_model(model_type)

        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")

        print("\n3. è®­ç»ƒæ¨¡å‹...")
        history = self.train_model(
            model, X_train, y_train, X_val, y_val,
            epochs=epochs, batch_size=128, lr=0.001
        )

        # 4. æ¨¡å‹è¯„ä¼°
        print("\n4. æ¨¡å‹æ€§èƒ½è¯„ä¼°...")
        model.eval()
        with torch.no_grad():
            y_pred_train = model(X_train).cpu().numpy()
            y_pred_val = model(X_val).cpu().numpy()

        # ç¡®ä¿é¢„æµ‹ç»“æœæ˜¯æ­£ç¡®çš„å½¢çŠ¶
        if y_pred_val.ndim == 1:
            y_pred_val = y_pred_val.reshape(-1, 1)
        if y_pred_val.shape[1] != self.output_dim:
            print(f"è­¦å‘Š: éªŒè¯é›†é¢„æµ‹å½¢çŠ¶ä¸æ­£ç¡®: {y_pred_val.shape}")
            if y_pred_val.shape[1] > self.output_dim:
                y_pred_val = y_pred_val[:, :self.output_dim]
            else:
                pad_size = self.output_dim - y_pred_val.shape[1]
                y_pred_val = np.pad(y_pred_val, ((0, 0), (0, pad_size)), mode='constant')

        # è®¡ç®—RÂ²åˆ†æ•°ï¼ˆä½¿ç”¨çœŸå®å€¼ï¼‰
        y_val_real = self.target_scaler.inverse_transform(y_val.cpu().numpy())
        y_pred_val_real = self.target_scaler.inverse_transform(y_pred_val)

        print("RÂ²å†³å®šç³»æ•° (è¶Šé«˜è¶Šå¥½):")
        for i, name in enumerate(self.perf_names):
            r2 = r2_score(y_val_real[:, i], y_pred_val_real[:, i])
            print(f"  {name}: {r2:.4f}")

        # 5. å¯è§†åŒ–ç»“æœ
        print("\n5. ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
        self.visualize_results(history, y_val.cpu().numpy(), y_pred_val, model_type)

        # 6. å¤©çº¿å‚æ•°ä¼˜åŒ–
        print("\n6. è´´ç‰‡å¤©çº¿å‚æ•°ä¼˜åŒ–...")

        # å®šä¹‰è®¾è®¡ç›®æ ‡
        target_specs = [
            -35.0,   # S11æœ€å°å€¼: -35dB
            2.45,    # å¯¹åº”é¢‘ç‡: 2.45GHz
            7.0      # è¿œåŒºåœºå¢ç›Š: 7.0dBi
        ]

        print(f"è®¾è®¡ç›®æ ‡:")
        for i, (name, target) in enumerate(zip(self.perf_names, target_specs)):
            print(f"  {name}: {target}")

        # å‚æ•°è¾¹ç•Œ - ä¿®æ”¹ï¼šåªæœ‰ä¸¤ä¸ªå‚æ•°çš„è¾¹ç•Œ
        param_min = X_original.min(axis=0)
        param_max = X_original.max(axis=0)
        param_bounds = np.column_stack([param_min, param_max])

        print(f"\nå‚æ•°ä¼˜åŒ–è¾¹ç•Œ:")
        for i, name in enumerate(self.param_names):
            print(f"  {name}: {param_bounds[i, 0]:.3f} - {param_bounds[i, 1]:.3f}")

        # æ‰§è¡Œä¼˜åŒ–
        optimal_params, predicted_performance, optimization_loss = self.optimize_antenna(
            model, target_specs, param_bounds, num_iterations=3000
        )

        print(f"\nä¼˜åŒ–ç»“æœ:")
        print(f"æœ€ä¼˜å‚æ•°:")
        for i, name in enumerate(self.param_names):
            print(f"  {name}: {optimal_params[i]:.3f}")

        print(f"\né¢„æµ‹æ€§èƒ½:")
        for i, name in enumerate(self.perf_names):
            diff = abs(predicted_performance[i] - target_specs[i])
            status = "âœ“" if (name == self.perf_names[0] and predicted_performance[i] <= target_specs[i]) or \
                           (name == self.perf_names[1] and abs(diff) < 0.05) or \
                           (name == self.perf_names[2] and predicted_performance[i] >= target_specs[i]) else "âš ï¸"
            print(f"  {status} {name}: {predicted_performance[i]:.3f} (ç›®æ ‡: {target_specs[i]})")

        # 7. HFSSä»¿çœŸéªŒè¯
        print(f"\n7. HFSSä»¿çœŸéªŒè¯...")
        simulated_performance = self.hfss_interface(optimal_params)

        # 8. è®¾è®¡å¯è¡Œæ€§åˆ†æ
        print(f"\n8. è®¾è®¡å¯è¡Œæ€§åˆ†æ:")
        is_feasible = True

        # S11æ£€æŸ¥
        if predicted_performance[0] > -15:
            print(f"  âš ï¸  S11å€¼ {predicted_performance[0]:.2f}dB åé«˜ï¼Œå¯èƒ½éœ€è¦æ”¹è¿›åŒ¹é…")
            is_feasible = False
        else:
            print(f"  âœ“ S11å€¼ {predicted_performance[0]:.2f}dB æ»¡è¶³è¦æ±‚")

        # é¢‘ç‡æ£€æŸ¥
        if not (2.4 <= predicted_performance[1] <= 2.5):
            print(f"  âš ï¸  å·¥ä½œé¢‘ç‡ {predicted_performance[1]:.2f}GHz ä¸åœ¨WiFi 2.4GHzé¢‘æ®µå†…")
            is_feasible = False
        else:
            print(f"  âœ“ å·¥ä½œé¢‘ç‡åœ¨WiFi 2.4GHzé¢‘æ®µå†…")

        # å¢ç›Šæ£€æŸ¥
        if predicted_performance[2] < 5.0:
            print(f"  âš ï¸  å¢ç›Š {predicted_performance[2]:.2f}dBi åä½")
            is_feasible = False
        else:
            print(f"  âœ“ å¢ç›Š {predicted_performance[2]:.2f}dBi æ»¡è¶³è¦æ±‚")

        end_time = time.time()
        print(f"\n=== è´´ç‰‡å¤©çº¿è®¾è®¡å·¥ä½œæµç¨‹å®Œæˆ ===")
        print(f"æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")

        if is_feasible:
            print("ğŸ‰ è®¾è®¡æˆåŠŸï¼è¯¥è´´ç‰‡å¤©çº¿è®¾è®¡æ»¡è¶³è¦æ±‚ã€‚")
        else:
            print("âš ï¸  è®¾è®¡åŸºæœ¬å®Œæˆï¼Œä½†éƒ¨åˆ†æŒ‡æ ‡éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚")

        # ä¿å­˜è®¾è®¡ç»“æœ
        design_result = {
            'optimal_parameters': optimal_params,
            'predicted_performance': predicted_performance,
            'simulated_performance': simulated_performance,
            'target_specifications': target_specs,
            'optimization_loss': optimization_loss,
            'model_type': model_type,
            'training_history': history,
            'is_feasible': is_feasible,
            'total_time': end_time - start_time,
            'r2_scores': [r2_score(y_val_real[:, i], y_pred_val_real[:, i]) for i in range(self.output_dim)]
        }

        np.save(f'patch_antenna_results/{model_type}_design_result.npy', design_result)
        print(f"è®¾è®¡ç»“æœå·²ä¿å­˜åˆ° patch_antenna_results/{model_type}_design_result.npy")

        return design_result

if __name__ == "__main__":
    # æ¼”ç¤ºä½¿ç”¨
    system = PatchAntennaDesignSystem()

    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    import sys
    if len(sys.argv) > 1 and sys.argv[1].endswith('.csv'):
        csv_file = sys.argv[1]
        print(f"ä½¿ç”¨CSVæ–‡ä»¶: {csv_file}")

        param_cols = None
        perf_cols = None
        if len(sys.argv) > 3:
            param_cols = sys.argv[2].split(',')
            perf_cols = sys.argv[3].split(',')

        result = system.design_workflow(
            csv_file=csv_file,
            param_cols=param_cols,
            perf_cols=perf_cols,
            model_type='resnet',
            epochs=300
        )
    else:
        # ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œæ¼”ç¤º
        print("ä½¿ç”¨åˆæˆæ•°æ®è¿›è¡Œæ¼”ç¤º (æ·»åŠ CSVæ–‡ä»¶è·¯å¾„ä½œä¸ºå‚æ•°å¯ä½¿ç”¨çœŸå®æ•°æ®)")

        # å¯ä»¥å°è¯•ä¸åŒçš„æ¨¡å‹ç±»å‹
        model_type = 'resnet'  # 'mlp', 'resnet', 'cnn', 'rnn', 'gnn'

        result = system.design_workflow(
            model_type=model_type,
            epochs=300,
            use_synthetic_data=True
        )

    print("\nè®¾è®¡æµç¨‹å…¨éƒ¨å®Œæˆï¼")